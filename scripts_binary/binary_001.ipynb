{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ashkan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ashkan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2023-08-23 11:22:07.862055: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-23 11:22:07.910157: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-23 11:22:08.643974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 11:22:09.781266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 11:22:09.836807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 11:22:09.836867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# file management\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import gensim\n",
    "import gensim.downloader as gdownload\n",
    "\n",
    "# deep learning\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "print(tf.config.list_physical_devices('GPU')) # check if gpu is detected\n",
    "from keras import backend as K\n",
    "import gc\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# visualization\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "# tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# performance\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email_ID</th>\n",
       "      <th>Sender</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>noreply@powerballs.com</td>\n",
       "      <td>You Have Won!</td>\n",
       "      <td>&lt;p&gt;*********PLEASE DO NOT RESPOND TO THIS EMAI...</td>\n",
       "      <td>Phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>noreply@paypalceo.com</td>\n",
       "      <td>PayPal Breach</td>\n",
       "      <td>&lt;p&gt;********* RESPONES TO THIS EMAIL WILL NOT B...</td>\n",
       "      <td>Phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>support@credit.chase.com</td>\n",
       "      <td>URGENT: Fraudulent activity detected</td>\n",
       "      <td>&lt;p&gt;Hello,&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;We are writ...</td>\n",
       "      <td>Phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>mary@yahoo.com</td>\n",
       "      <td>Donations needed for Mark</td>\n",
       "      <td>&lt;p&gt;Hello,&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;I&amp;#39;m contactin...</td>\n",
       "      <td>Phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>support@security.amazon.com</td>\n",
       "      <td>Your Amazon Account</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;The account number associated with ...</td>\n",
       "      <td>Phishing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Email_ID                       Sender  \\\n",
       "0         1       noreply@powerballs.com   \n",
       "1         2        noreply@paypalceo.com   \n",
       "2         3     support@credit.chase.com   \n",
       "3         4               mary@yahoo.com   \n",
       "4         5  support@security.amazon.com   \n",
       "\n",
       "                                Subject  \\\n",
       "0                         You Have Won!   \n",
       "1                         PayPal Breach   \n",
       "2  URGENT: Fraudulent activity detected   \n",
       "3             Donations needed for Mark   \n",
       "4                  Your Amazon Account    \n",
       "\n",
       "                                               Email Email_type  \n",
       "0  <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing  \n",
       "1  <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing  \n",
       "2  <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing  \n",
       "3  <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing  \n",
       "4  <p><strong>The account number associated with ...   Phishing  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Phishing', 'Ham'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('./PhishingDataset_HFES2020.csv', encoding='windows-1252')\n",
    "df = df.replace('ham', 'Ham') # replace ham with Ham\n",
    "df = df[df['Email_type'] != 'Attention_check'] # remove Attention_check s\n",
    "display(df.head())\n",
    "display(df['Email_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessin funcs\n",
    "# preprocessing functions\n",
    "\n",
    "def multiple_replace(arr, replace, source):\n",
    "    for item in arr:\n",
    "        source = re.sub(item, replace, source)\n",
    "\n",
    "    return source\n",
    "\n",
    "def text_preprocessing(tweets_list, embedding, maxlen):\n",
    "    set_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    processed_tweets = []\n",
    "    for i, txt in enumerate(tweets_list):        \n",
    "        # replace stuff\n",
    "        txt = re.sub(r'\\b\\S*[\\x80-\\xFF]\\S*\\b', ' ', txt) # any words with non-ascii characters\n",
    "        txt = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', ' url ', txt) # urls\n",
    "        txt = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', ' email ', txt) # emails\n",
    "        txt = re.sub(r'<.*?>', '', txt) # remove html tags completely\n",
    "        txt = re.sub(r'&.*?;', ' ', txt) # remove HTML entities\n",
    "        txt = re.sub(r'#', ' ', txt) # hastags --> just remove the tag\n",
    "        txt = re.sub(r'\\b\\d+\\b', ' num ', txt) # numbers\n",
    "        txt = re.sub(r'[^\\w\\s]', r' \\g<0> ', txt) # punctuation\n",
    "        \n",
    "        # lowercase\n",
    "        txt = txt.lower()\n",
    "\n",
    "        # https://saturncloud.io/blog/reshaping-text-data-for-lstm-models-in-keras-a-comprehensive-guide/\n",
    "\n",
    "        # split\n",
    "        # nltk handles all punctuation as features\n",
    "        word_arr = re.split(f'\\s+', txt) # returns list of words\n",
    "    \n",
    "        # remove stopwords and drop empty strings\n",
    "        word_arr = [word for word in word_arr if word not in set_stopwords and len(word) != 0]\n",
    "        \n",
    "        # lemmatize\n",
    "        lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        word_arr = [lemmatizer.lemmatize(word) for word in word_arr]\n",
    "\n",
    "        if i % 10000 == 0: # log the processed message in specified intervals\n",
    "            print(f\"Processed text #{i}:\", word_arr)\n",
    "            print(\"---------------------------\")\n",
    "\n",
    "        processed_tweets.append(word_arr)\n",
    "    \n",
    "    # tokenize (I ditched the old tokenizer)\n",
    "    print(\"tokenizing...\")\n",
    "    embedding_length = len(embedding)\n",
    "    # convert each word to its index. if it doesn't exist, set it to the last index. I don't care that it ruins one word's meaning\n",
    "    tokenized = [[embedding.key_to_index[word] if word in embedding else (embedding_length - 1) for word in split_sentence] for split_sentence in processed_tweets]\n",
    "\n",
    "    # add padding and convert to numpy array\n",
    "    print('padding sequences...')\n",
    "    tokenized = np.asarray(keras.preprocessing.sequence.pad_sequences(\n",
    "            tokenized,\n",
    "            padding = 'post',\n",
    "            maxlen = maxlen,\n",
    "    ))\n",
    "\n",
    "    # DEBUG\n",
    "    print(tokenized)\n",
    "    print('feature vector shape:', tokenized.shape)\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# preprocess annotations for initial binary classification\n",
    "def annotation_preprocessing(annotation_list):\n",
    "    # set all \"threat\" to 1, the rest to 0\n",
    "    return np.asarray([1 if x == \"Phishing\" else 0 for x in annotation_list])\n",
    "\n",
    "def train_valid_test_split(ds, train_ratio, valid_ratio, batch_size, k = None):\n",
    "    train_ratio = 0.8\n",
    "    valid_ratio = 0.1\n",
    "    init_len = len(ds)\n",
    "    num_train = np.floor(init_len * train_ratio)\n",
    "    num_valid = np.floor(init_len * valid_ratio)\n",
    "\n",
    "    train_ds = ds.take(num_train).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    valid_ds = ds.skip(num_train).take(num_valid).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = ds.skip(num_train).skip(num_valid).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    print(f'train ds has {num_train} items in {len(train_ds)} batches.')\n",
    "    print(f'valid ds has {num_valid} items in {len(valid_ds)} batches.')\n",
    "    print(f'test ds has {init_len - num_train - num_valid} items in {len(test_ds)} batches.')\n",
    "\n",
    "    return (train_ds, valid_ds, test_ds)\n",
    "\n",
    "def train_and_evaluate(model, train_ds, test_ds, epochs, optimizer, loss, valid_ds = None):\n",
    "    model.compile(\n",
    "        loss = loss,\n",
    "        optimizer = optimizer,\n",
    "        metrics = ['acc'],\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data = valid_ds, # ignored if None\n",
    "        epochs = epochs,\n",
    "    )\n",
    "\n",
    "    if valid_ds != None:\n",
    "        # plot losses over time --> shown after training\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.xlabel('accuracy')\n",
    "        plt.legend(['train','val'], loc='upper left')\n",
    "        plt.grid()\n",
    "        plt.ylim(0.5, 1)\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.xlabel('loss')\n",
    "        plt.legend(['train','val'], loc='upper left')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    # evaluate\n",
    "    return model.evaluate(test_ds)[1]\n",
    "\n",
    "def tokenize_types(unique_types):\n",
    "    index_to_type = {index: value for index, value in zip(range(len(unique_types)), unique_types)}\n",
    "    type_to_index = {value: index for index, value in index_to_type.items()}\n",
    "    return (index_to_type, type_to_index)\n",
    "\n",
    "def types_preprocessing(typescol):\n",
    "    # notes: \n",
    "    # I'll just use the first vulnerability type \n",
    "    # If the type is \"all\", I'll drop it from the input\n",
    "\n",
    "    processed = []\n",
    "    for string in typescol:\n",
    "        # print(string)\n",
    "        string = string.lower() # lowercase\n",
    "        string = re.sub(r\"[,\\[\\]']\", \" \", string) # replace [, ], and ' with space\n",
    "        strlist = string.split() # split list by space\n",
    "        if 'all' in strlist: strlist.remove('all') # remove the \"all\" type\n",
    "        one_type = strlist[0] if len(strlist) > 0 else \"empty\" # select just the first type\n",
    "\n",
    "        processed.append(one_type)\n",
    "\n",
    "    index_to_type, type_to_index = tokenize_types(set(processed))\n",
    "    # print(type_to_index.items()) # DEBUG\n",
    "\n",
    "    for i in range(len(processed)):\n",
    "        processed[i] = type_to_index[processed[i]] # convert each type to index\n",
    "\n",
    "    return (index_to_type, type_to_index, np.asarray(processed))\n",
    "\n",
    "def shuffle(nparr, random_state = 23):\n",
    "    rng = np.random.RandomState(random_state) # reset the seed\n",
    "    return rng.permutation(nparr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the average length of emails\n",
    "# tweetlengths = df['Email'].apply(lambda x: len(x.split()))\n",
    "# print(np.average(tweetlengths))\n",
    "# # I'll use 100 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdownload.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text #0: ['*', '*', '*', '*', '*', '*', '*', '*', '*', 'please', 'respond', 'email', '*', '*', '*', '*', '*', '*', '*', '*', '*', 'record', 'show', 'entered', 'win', 'state', 'powerball', 'jackpot', 'num', '/', 'num', '/', 'num', '.', 'receiving', 'email', 'listed', 'one', 'winner', '.', 'claim', 'prize', 'please', 'visit', 'site', 'fill', 'information', 'needed', 'collect', '.', 'must', 'process', 'information', 'within', 'week', 'time', 'may', 'lose', 'winning', '.', 'congratulation', '!', 'collect', 'earnings', '!', 'please', 'click', 'prompt', 'response', 'regarding', 'matter', 'appreciated', '.', 'sincerely', ',', 'powerball', 'team']\n",
      "---------------------------\n",
      "tokenizing...\n",
      "padding sequences...\n",
      "[[   42    42    42 ...     0     0     0]\n",
      " [   42    42    42 ...     0     0     0]\n",
      " [  996     4  3145 ...     0     0     0]\n",
      " ...\n",
      " [  589 31134     4 ...     0     0     0]\n",
      " [    1  3709     1 ...  2471  2311     1]\n",
      " [  589     4 11532 ...     0     0     0]]\n",
      "feature vector shape: (239, 80)\n",
      "original data:\n",
      "<<0>> <p><strong>17/10/2017<br />Account No: 108-455294-800125-MN</strong></p><p>&nbsp;</p><p>To our valued <em>Walmart</em> customer,&nbsp;</p><p>&nbsp;</p><p>The team at <em>Walmart Online Services</em> is happy to announce that you have been chosen as the &#39;User of the month&#39; lottery&nbsp;winner!&nbsp;This is a monthly event where we randomly choose a customer to receive&nbsp;free Walmart coupons. In order to&nbsp;access your coupons, please visit the link below:</p><p>&nbsp;</p><p><strong><a href=\"http://walmart/rewards/coupons\" onclick=\"return false;\">Walmart Reward Coupons</a></strong></p><p>&nbsp;</p><p>Using any of these coupons will result in a donation to St. Jude&#39;s Children Hospital, so you can now receive free items and donate to a good cause at the same time. We hope you&#39;ll help support us!<br /><br /><em><strong>From,</strong></em></p><p><strong>Your Friends at Walmart Online Services</strong></p><p>&nbsp;</p>\n",
      "<<1>> <p>What would&nbsp;you do with extra money each month? &nbsp;</p><p>&nbsp;</p><p>JP Morgan&nbsp;is offering a new 30 Year Fixed Term Loan with rates starting from <em><strong>1.05%</strong></em> to qualified customers. Its easy to see if you qualify! Don&#39;t let this opportunity pass you by!</p><p>&nbsp;</p><p>To see if you qualify, please click the link below.</p><p>&nbsp;</p><p><a target=\"\" href=\"http://Chase-Security.html\" onclick=\"return false;\">https://chaseonline.com/promotional/30YearJumbo/qualification.aspx</a>&nbsp;</p><p>&nbsp;</p><p>We look forward to hearing from&nbsp;you,&nbsp;</p><p>&nbsp;</p><p>Sincerely,&nbsp;</p><p>JP Morgan Chase Bank, N.A.</p>\n",
      "<<2>> <p>Dear Loyal Customer:</p><p>&nbsp;</p><p>JP Morgan Chase Bank is excited to announce their&nbsp;30 Year Fixed Term Jumbo Loan with rates starting from 1.05% to qualified customers to refinance or get a loan for a house</p><p>&nbsp;</p><p>In only seconds you can see if you qualify, just click here. We have representives standing by.&nbsp;</p><p>&nbsp;</p><p><a target=\"\" href=\"http://Chase-Security.html\" onclick=\"return false;\">https://chaseonline.com/promotional/30YearJumbo/qualification.aspx</a>&nbsp;</p><p>&nbsp;</p><p>We look forward to serving all of your financial needs.&nbsp;</p><p>&nbsp;</p><p>Sincerely,&nbsp;</p><p>JP Morgan Chase Bank, N.A.</p><p>Chase Privacy Operations, P.O. Box 659752, San Antonio, TX 78265-9752.&nbsp;</p><p>&copy; 2016 JPMorgan Chase &amp; Co.</p>\n",
      "<<3>> <p>Dear Sir or Madam:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; We are required by law to contact you with the following information. You account is in need of prompt attention. With one click you will be able to update your account&nbsp;and continue using our simple, no risk website to continue with all your business needs. We encourage you to do this as quickly as possible. Without further action on your part, we will find it necessary to close this account.</p><p><a href=\"http://www.USSAgoogle.com\" onclick=\"return false;\">&nbsp;Please click this sentence</a></p><p>&nbsp;</p><p>Thank you for banking with us.</p><p>&nbsp;</p><p>Sincerely,</p><p>USAA Bank</p>\n",
      "<<4>> <p>To Whom It May Concern:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I possess a Master&#39;s Degree in Accounting, and am currently in the process of completing my CPA qualifications. I also have advanced experience&nbsp;in Microsoft Access, Excel, Word and Powerpoint, QuickBooks, and other accounting software. I believe that I could be a major asset to your company.</p><p>&nbsp;</p><p>Please take the time to look through my qualifications:</p><p><a target=\"_blank\" href=\"https://gmail.com/myemail\" onclick=\"return false;\">My CV</a></p><p>&nbsp;</p><p>I thank you for your time and hope that you will consider me for a position.</p><p>&nbsp;</p><p>Best,</p><p>Ryan Walters</p>\n",
      "<<5>> <p>&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; I am having some serious medical issues and could use your help cousin :(. Please donate to me on the page below and give me a call when I&#39;m all better.&nbsp;</p><p><a href=\"http://www.dfnbk.com\" onclick=\"return false;\">Link!</a></p>\n",
      "<<6>> <p>Hello,</p><p>&nbsp;</p><p>Thank you for shopping with us.</p><p>We will email you once your item&nbsp;has shipped.</p><p>&nbsp;</p><p>Order Details:</p><p>Order # 002-527657-2343090</p><p>Shipping: FREE Two-Day Shipping</p><p>Order total: $109.29</p><p>&nbsp;</p><p>If you didn&rsquo;t authorize the transaction, please <a href=\"http://www.refund.item\" onclick=\"return false;\" > Click Here </a>to receive a full refund.</p><p>&nbsp;</p><p>We hope to see you again soon.</p><p>&nbsp;</p><p>Amazon.com</p>\n",
      "<<7>> <p>&nbsp;</p><p>&nbsp;</p><p>We are writing to you because your federal Tax refund&nbsp;is now ready to be sent. To confirm your account please visit the website below.&nbsp;</p><p>If verification is not&nbsp;made within the next 72 hours your refund could substantially be delayed. Log in now to secure your refund.&nbsp;</p><p>&nbsp;</p><p>&nbsp;<a href=\"http://www.IR5.com\" onclick=\"return false;\">www.irs.com</a> &nbsp;</p><p>&nbsp;</p><p>Again, please respond within the next 72 hours to avoid delays to your refund.</p><p>&nbsp;</p><p>Sincerely,</p><p>&nbsp;</p><p>IRS Refund Department</p>\n",
      "<<8>> <p><strong>Your Account has been locked! You can resolve this now.</strong>&nbsp;</p><p>&nbsp;</p><p>Case id : 9000321-128. Login attempt from unknown device.&nbsp;</p><p>&nbsp;</p><p><strong>Dear Client&nbsp;</strong></p><p>Someone has accessed your account, so we have temporarily locked it to keep your personal informations in safe. To unlock your account, you may need to pass a security check. Note that attempting to access someone else is a violation of PayPals terms. It may also be illegal. To reset your account:&nbsp;</p><p>1-Click on the link <a target=\"\" href=\"PayPal1-Security.html\" onclick=\"return false;\">PayPal-Security.html</a>.&nbsp;</p><p>2-Open the page in a browser window secure.&nbsp;</p><p>3-Follow the instructions.</p><p>&nbsp;</p><p>&nbsp;</p>\n",
      "<<9>> <h4>Add a backup phone as an alternative 2-step Verification step</h4>andrew19@gmail.com<br><br>A backup phone number ensures that you can sign in when your main second verification step is unavailable.<br><br>Add a backup number and see other personalised recommendations in the Security Check-up.<h4><a href=\"https://www.gmail.com\" onclick=\"return false;\">TAKE ACTION </a></h4>\n",
      "------------------------------------------\n",
      "split input: \n",
      "<<0>> ['num', '/', 'num', '/', 'ﾟﾟﾟｵﾔｽﾐｰ', ':', 'num', '-', 'num', '-', 'num', '-', 'mn', 'valued', 'walmart', 'customer', ',', 'team', 'walmart', 'online', 'service', 'happy', 'announce', 'chosen', 'user', 'month', 'lottery', 'winner', '!', 'monthly', 'event', 'randomly', 'choose', 'customer', 'receive', 'free', 'walmart', 'coupon', '.', 'order', 'access', 'coupon', ',', 'please', 'visit', 'link', ':', 'walmart', 'reward', 'coupon', 'using', 'coupon', 'result', 'donation', 'st', '.', 'jude', 'child', 'hospital', ',', 'receive', 'free', 'item', 'donate', 'good', 'cause', 'time', '.', 'hope', 'help', 'support', 'u', '!', ',', 'friend', 'walmart', 'online', 'service', '<user>', '<user>']\n",
      "<<1>> ['would', 'extra', 'money', 'month', '?', 'jp', 'morgan', 'offering', 'new', 'num', 'year', 'fixed', 'term', 'loan', 'rate', 'starting', 'num', '.', 'num', '%', 'qualified', 'customer', '.', 'easy', 'see', 'qualify', '!', 'let', 'opportunity', 'pas', '!', 'see', 'qualify', ',', 'please', 'click', 'link', '.', 'url', 'look', 'forward', 'hearing', ',', 'sincerely', ',', 'jp', 'morgan', 'chase', 'bank', ',', 'n', '.', '.', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<2>> ['dear', 'loyal', 'customer', ':', 'jp', 'morgan', 'chase', 'bank', 'excited', 'announce', 'num', 'year', 'fixed', 'term', 'jumbo', 'loan', 'rate', 'starting', 'num', '.', 'num', '%', 'qualified', 'customer', 'refinance', 'get', 'loan', 'house', 'second', 'see', 'qualify', ',', 'click', '.', 'ﾟﾟﾟｵﾔｽﾐｰ', 'standing', '.', 'url', 'look', 'forward', 'serving', 'financial', 'need', '.', 'sincerely', ',', 'jp', 'morgan', 'chase', 'bank', ',', 'n', '.', '.', 'chase', 'privacy', 'operation', ',', 'p', '.', '.', 'box', 'num', ',', 'san', 'antonio', ',', 'tx', 'num', '-', 'num', '.', 'num', 'jpmorgan', 'chase', 'co', '.', '<user>', '<user>', '<user>']\n",
      "<<3>> ['dear', 'sir', 'madam', ':', 'required', 'law', 'contact', 'following', 'information', '.', 'account', 'need', 'prompt', 'attention', '.', 'one', 'click', 'able', 'update', 'account', 'continue', 'using', 'simple', ',', 'risk', 'website', 'continue', 'business', 'need', '.', 'encourage', 'quickly', 'possible', '.', 'without', 'action', 'part', ',', 'find', 'necessary', 'close', 'account', '.', 'please', 'click', 'sentence', 'thank', 'banking', 'u', '.', 'sincerely', ',', 'usaa', 'bank', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<4>> ['may', 'concern', ':', 'posse', 'master', 'degree', 'accounting', ',', 'currently', 'process', 'completing', 'cpa', 'qualification', '.', 'also', 'advanced', 'experience', 'microsoft', 'access', ',', 'excel', ',', 'word', 'powerpoint', ',', 'quickbooks', ',', 'accounting', 'software', '.', 'believe', 'could', 'major', 'asset', 'company', '.', 'please', 'take', 'time', 'look', 'qualification', ':', 'cv', 'thank', 'time', 'hope', 'consider', 'position', '.', 'best', ',', 'ryan', 'walter', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<5>> ['serious', 'medical', 'issue', 'could', 'use', 'help', 'cousin', ':', '(', '.', 'please', 'donate', 'page', 'give', 'call', 'better', '.', 'link', '!', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<6>> ['hello', ',', 'thank', 'shopping', 'u', '.', 'email', 'item', 'shipped', '.', 'order', 'detail', ':', 'order', 'num', '-', 'num', '-', 'ﾟﾟﾟｵﾔｽﾐｰ', ':', 'free', 'two', '-', 'day', 'ﾟﾟﾟｵﾔｽﾐｰ', 'total', ':', '$', 'num', '.', 'num', 'authorize', 'transaction', ',', 'please', 'click', 'receive', 'full', 'refund', '.', 'hope', 'see', 'soon', '.', 'url', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<7>> ['writing', 'federal', 'tax', 'refund', 'ready', 'sent', '.', 'confirm', 'account', 'please', 'visit', 'website', '.', 'verification', 'made', 'within', 'next', 'num', 'hour', 'refund', 'could', 'substantially', 'delayed', '.', 'log', 'secure', 'refund', '.', 'url', ',', 'please', 'respond', 'within', 'next', 'num', 'hour', 'avoid', 'delay', 'refund', '.', 'sincerely', ',', 'irs', 'refund', 'department', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<8>> ['account', 'locked', '!', 'resolve', '.', 'case', 'id', ':', 'num', '-', 'num', '.', 'login', 'attempt', 'unknown', 'device', '.', 'dear', 'client', 'someone', 'accessed', 'account', ',', 'temporarily', 'locked', 'keep', 'personal', 'information', 'safe', '.', 'unlock', 'account', ',', 'may', 'need', 'pas', 'security', 'check', '.', 'note', 'attempting', 'access', 'someone', 'else', 'violation', 'ﾟﾟﾟｵﾔｽﾐｰ', 'term', '.', 'may', 'also', 'illegal', '.', 'reset', 'account', ':', 'num', '-', 'click', 'link', 'url', '.', 'num', '-', 'open', 'page', 'browser', 'window', 'secure', '.', 'num', '-', 'follow', 'instruction', '.', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<9>> ['add', 'backup', 'phone', 'alternative', 'num', '-', 'step', 'verification', 'step', 'url', 'backup', 'phone', 'number', 'ensures', 'sign', 'main', 'second', 'verification', 'step', 'unavailable', '.', 'add', 'backup', 'number', 'see', 'personalised', 'recommendation', 'security', 'check', '-', '.', 'take', 'action', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "------------------------------------------\n",
      "tokenized input: \n",
      "<<0>> [3709, 38, 3709, 38, 1193513, 2, 3709, 28, 3709, 28, 3709, 28, 2886, 54234, 7214, 7922, 4, 302, 7214, 949, 2465, 177, 8261, 12628, 3920, 1423, 16634, 3104, 9, 21024, 2466, 8209, 2665, 7922, 6398, 424, 7214, 12285, 1, 1853, 7530, 12285, 4, 200, 1621, 1345, 2, 7214, 15037, 12285, 1802, 12285, 5678, 23473, 993, 1, 24751, 1916, 2974, 4, 6398, 424, 6657, 8223, 117, 507, 135, 1, 420, 515, 1215, 51, 9, 4, 531, 7214, 949, 2465, 0, 0]\n",
      "<<1>> [196, 2052, 580, 1423, 14, 9182, 6711, 13830, 122, 3709, 356, 8090, 6261, 9621, 3760, 1752, 3709, 1, 3709, 299, 28111, 7922, 1, 1227, 163, 30517, 9, 265, 5571, 354, 9, 163, 30517, 4, 200, 2215, 1345, 1, 14119, 273, 1732, 3856, 4, 11404, 4, 9182, 6711, 5037, 2699, 4, 36, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<2>> [1034, 3972, 7922, 2, 9182, 6711, 5037, 2699, 870, 8261, 3709, 356, 8090, 6261, 28257, 9621, 3760, 1752, 3709, 1, 3709, 299, 28111, 7922, 158204, 87, 9621, 543, 1296, 163, 30517, 4, 2215, 1, 1193513, 4454, 1, 14119, 273, 1732, 15269, 8617, 171, 1, 11404, 4, 9182, 6711, 5037, 2699, 4, 36, 1, 1, 5037, 13958, 13787, 4, 351, 1, 1, 2271, 3709, 4, 1401, 6683, 4, 7612, 3709, 28, 3709, 1, 3709, 98039, 5037, 2384, 1, 0, 0, 0]\n",
      "<<3>> [1034, 2621, 24102, 2, 13544, 3187, 3514, 927, 5386, 1, 1548, 171, 62081, 1823, 1, 96, 2215, 1773, 1880, 1548, 3475, 1802, 1617, 4, 5889, 2529, 3475, 1249, 171, 1, 19253, 5935, 2436, 1, 663, 3196, 802, 4, 470, 10008, 1213, 1548, 1, 200, 2215, 8363, 337, 24686, 51, 1, 11404, 4, 276017, 2699, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<4>> [530, 14730, 2, 47592, 4333, 9310, 21018, 4, 3996, 7014, 51524, 38391, 78372, 1, 894, 16857, 3084, 6506, 7530, 4, 24064, 4, 892, 33943, 4, 178673, 4, 21018, 7287, 1, 552, 297, 3577, 39541, 3054, 1, 200, 284, 135, 273, 78372, 2, 9027, 337, 135, 420, 6213, 5956, 1, 209, 4, 2827, 16956, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<5>> [1507, 7294, 4649, 297, 716, 515, 2569, 2, 17, 1, 200, 8223, 1737, 374, 462, 295, 1, 1345, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<6>> [996, 4, 337, 1652, 51, 1, 2600, 6657, 27955, 1, 1853, 13794, 2, 1853, 3709, 28, 3709, 28, 1193513, 2, 424, 568, 28, 125, 1193513, 2413, 2, 206, 3709, 1, 3709, 284862, 80102, 4, 200, 2215, 6398, 833, 31845, 1, 420, 163, 698, 1, 14119, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<7>> [3145, 8677, 5098, 31845, 579, 2089, 1, 9255, 1548, 200, 1621, 2529, 1, 80719, 425, 4615, 411, 3709, 1194, 31845, 297, 239791, 18502, 1, 8253, 17559, 31845, 1, 14119, 4, 200, 7093, 4615, 411, 3709, 1194, 5839, 9023, 31845, 1, 11404, 4, 21039, 31845, 13272, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<8>> [1548, 7054, 9, 18309, 1, 1815, 2471, 2, 3709, 28, 3709, 1, 25037, 7990, 9697, 13249, 1, 1034, 13322, 238, 264007, 1548, 4, 30878, 7054, 416, 2615, 5386, 2184, 1, 17527, 1548, 4, 530, 171, 354, 4739, 525, 1, 2586, 22676, 7530, 238, 645, 49497, 1193513, 6261, 1, 530, 894, 7188, 1, 24401, 1548, 2, 3709, 28, 2215, 1345, 14119, 1, 3709, 28, 784, 1737, 24107, 3962, 17559, 1, 3709, 28, 84, 58103, 1, 0, 0, 0, 0, 0, 0]\n",
      "<<9>> [1894, 16464, 448, 15792, 3709, 28, 2022, 80719, 2022, 14119, 16464, 448, 1077, 148726, 1793, 1029, 1296, 80719, 2022, 60682, 1, 1894, 16464, 1077, 163, 67974, 51557, 4739, 525, 28, 1, 284, 3196, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "------------------------------------------\n",
      "labels:  [1 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# actual preprocessing\n",
    "# embedding = gdownload.load('glove-wiki-gigaword-100') # pretrained embedding --> much cleaner than twitter stuff\n",
    "embedding = gdownload.load('glove-twitter-200') # pretrained embedding \n",
    "# embedding = gdownload.load('glove-twitter-50') # pretrained embedding \n",
    "# embedding = gdownload.load('word2vec-google-news-300')\n",
    "original_texts = np.asarray(df['Email']) # I'll use this to check the preprocessing process\n",
    "\n",
    "max_words = 80\n",
    "email_bodies = text_preprocessing(df['Email'], embedding, maxlen = max_words) \n",
    "annotation_labels = annotation_preprocessing(df['Email_type'])\n",
    "# index_to_type, type_to_index, type_labels = types_preprocessing(df['type'])\n",
    "\n",
    "# shuffle the data\n",
    "\n",
    "###### vvvv SEED IS HERE vvvv ######\n",
    "# seed = 183\n",
    "# seed = 89\n",
    "# seed = 11\n",
    "# seed = 42\n",
    "seed = 30\n",
    "###### ^^^^ SEED IS HERE ^^^^ ######\n",
    "\n",
    "original_texts = shuffle(original_texts, random_state = seed) # debug\n",
    "email_bodies = shuffle(email_bodies, random_state = seed)\n",
    "annotation_labels = shuffle(annotation_labels, random_state = seed)\n",
    "# type_labels = shuffle(type_labels)\n",
    "\n",
    "# reduce data for faster training # REMOVE LATER\n",
    "ratio_keep = 1 \n",
    "original_texts = original_texts[:int(len(original_texts) * ratio_keep)] # debug\n",
    "email_bodies = email_bodies[:int(len(email_bodies) * ratio_keep)]\n",
    "annotation_labels = annotation_labels[:int(len(annotation_labels) * ratio_keep)]\n",
    "# type_labels = type_labels[:int(len(type_labels) * ratio_keep)]\n",
    "\n",
    "# DEBUG\n",
    "def print_list(title, list):\n",
    "    print(title)\n",
    "    for i, x in enumerate(list):\n",
    "        print(f'<<{i}>>', x)\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "sample_length = 10\n",
    "print_list(\"original data:\", original_texts[:sample_length])\n",
    "tokenized_input_sample = [[index for index in x] for x in email_bodies][:sample_length]\n",
    "print_list(\"split input: \", [[embedding.index_to_key[index] for index in example] for example in tokenized_input_sample])\n",
    "print_list(\"tokenized input: \", tokenized_input_sample)\n",
    "print(\"labels: \", annotation_labels[:sample_length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training funcs + k-fold\n",
    "\n",
    "def pretrained_embedding(embedding):\n",
    "    # note: embedding is declared in the previous cell\n",
    "    \n",
    "    vocab_size = len(embedding)\n",
    "    embedding_vector_size = len(embedding[embedding.index_to_key[0]])\n",
    "\n",
    "    # create embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_vector_size))\n",
    "    # iterate through embedding and copy word vectors as weights\n",
    "    for i in range(vocab_size):\n",
    "        embedding_matrix[i, :] = embedding[embedding.index_to_key[i]]\n",
    "\n",
    "    embedding_layer = layers.Embedding(input_dim = vocab_size, output_dim = embedding_vector_size, trainable = False)\n",
    "    embedding_layer.build((None,)) # I have no idea why I should do this\n",
    "    embedding_layer.set_weights([embedding_matrix]) # square brackets are because some layers take multiple types of weights\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Input(shape = (max_words,)),\n",
    "    pretrained_embedding(embedding),\n",
    "    # layers.GRU(MAX_TWEET_WORDS, return_sequences=True), # not a difference\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Bidirectional(\n",
    "        layers.GRU(\n",
    "            max_words,\n",
    "            dropout = 0.2,\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    layers.Dense(32, activation = 'relu'),\n",
    "    layers.Dropout(0.7),\n",
    "    layers.Dense(8, activation = 'relu'),\n",
    "    layers.Dropout(0.7),\n",
    "    layers.Dense(1, activation = 'sigmoid'),\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "def kfold(ds, epochs, batch_size, k):\n",
    "  loss = keras.losses.BinaryCrossentropy()\n",
    "  optimizer = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "  autotune = tf.data.AUTOTUNE\n",
    "\n",
    "  if k == None:\n",
    "    # normal stuff\n",
    "    model = build_model()\n",
    "\n",
    "    train_ds, valid_ds, test_ds = train_valid_test_split(ds, 0.6, 0.2, batch_size)\n",
    "    train_and_evaluate(\n",
    "      model,\n",
    "      train_ds = train_ds,\n",
    "      valid_ds = valid_ds,\n",
    "      test_ds = test_ds,\n",
    "      epochs = epochs,\n",
    "      loss = loss,\n",
    "      optimizer = optimizer,\n",
    "    )\n",
    "\n",
    "  else:\n",
    "    accuracies = []\n",
    "    for i in range(k):\n",
    "      print(f'fold {i}')\n",
    "      \n",
    "      model = build_model()\n",
    "      num_total = len(ds)\n",
    "      num_test = np.floor(num_total / k)\n",
    "      num_train = num_total - num_test\n",
    "\n",
    "      test_range = [np.floor((i) * num_test), np.floor((i + 1) * num_test)]\n",
    "      train_ds_p1 = ds.take(test_range[0])\n",
    "      train_ds_p2 = ds.skip(test_range[1])\n",
    "      train_ds = train_ds_p1.concatenate(train_ds_p2).batch(batch_size).prefetch(autotune)\n",
    "      print(f'train dataset range: {test_range[0]} - {test_range[1]}')\n",
    "      test_ds = ds.skip(np.floor((i) * num_test)).take(num_test).batch(batch_size).prefetch(autotune)\n",
    "      print(f'test dataset range: {test_range[0]} - {test_range[1]}')\n",
    "\n",
    "      print(f'train ds has {num_train} items in {len(train_ds)} batches.')\n",
    "      print(f'test ds has {num_test} items in {len(test_ds)} batches.')\n",
    "      \n",
    "      accuracy = train_and_evaluate(\n",
    "        model,\n",
    "        train_ds,\n",
    "        test_ds,\n",
    "        epochs = epochs,\n",
    "        loss = loss,\n",
    "        optimizer = optimizer,\n",
    "      )\n",
    "      \n",
    "      print(\"accuracy: \", accuracy)\n",
    "      accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"average accuracy: {np.average(accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 11:23:43.419201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 11:23:43.419492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 11:23:43.419522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 11:23:44.160769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 11:23:44.160856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 11:23:44.160863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-23 11:23:44.160892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 11:23:44.160929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3413 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ds has 191.0 items in 6 batches.\n",
      "valid ds has 23.0 items in 1 batches.\n",
      "test ds has 25.0 items in 1 batches.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 80, 200)           238702800 \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 80, 200)           800       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 160)               135360    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                5152      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 238844385 (911.12 MB)\n",
      "Trainable params: 141185 (551.50 KB)\n",
      "Non-trainable params: 238703200 (910.58 MB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 11:23:50.929452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-08-23 11:23:51.090742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-23 11:23:51.096246: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc37402d250 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-23 11:23:51.096288: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-08-23 11:23:51.101613: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-23 11:23:51.205027: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-08-23 11:23:51.274740: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model1_ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((email_bodies, annotation_labels))\n\u001b[0;32m----> 2\u001b[0m kfold(ds \u001b[39m=\u001b[39;49m model1_ds, epochs \u001b[39m=\u001b[39;49m \u001b[39m200\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m32\u001b[39;49m, k \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# note: I have serious memory leak problems with k-fold.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# I'll use the following seeds to verify the average accuracy:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# 183, 89, 11, 42, 30\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 53\u001b[0m, in \u001b[0;36mkfold\u001b[0;34m(ds, epochs, batch_size, k)\u001b[0m\n\u001b[1;32m     50\u001b[0m   model \u001b[39m=\u001b[39m build_model()\n\u001b[1;32m     52\u001b[0m   train_ds, valid_ds, test_ds \u001b[39m=\u001b[39m train_valid_test_split(ds, \u001b[39m0.6\u001b[39m, \u001b[39m0.2\u001b[39m, batch_size)\n\u001b[0;32m---> 53\u001b[0m   train_and_evaluate(\n\u001b[1;32m     54\u001b[0m     model,\n\u001b[1;32m     55\u001b[0m     train_ds \u001b[39m=\u001b[39;49m train_ds,\n\u001b[1;32m     56\u001b[0m     valid_ds \u001b[39m=\u001b[39;49m valid_ds,\n\u001b[1;32m     57\u001b[0m     test_ds \u001b[39m=\u001b[39;49m test_ds,\n\u001b[1;32m     58\u001b[0m     epochs \u001b[39m=\u001b[39;49m epochs,\n\u001b[1;32m     59\u001b[0m     loss \u001b[39m=\u001b[39;49m loss,\n\u001b[1;32m     60\u001b[0m     optimizer \u001b[39m=\u001b[39;49m optimizer,\n\u001b[1;32m     61\u001b[0m   )\n\u001b[1;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   accuracies \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[3], line 97\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_ds, test_ds, epochs, optimizer, loss, valid_ds)\u001b[0m\n\u001b[1;32m     90\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     91\u001b[0m     loss \u001b[39m=\u001b[39m loss,\n\u001b[1;32m     92\u001b[0m     optimizer \u001b[39m=\u001b[39m optimizer,\n\u001b[1;32m     93\u001b[0m     metrics \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     94\u001b[0m )\n\u001b[1;32m     96\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39msummary())\n\u001b[0;32m---> 97\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     98\u001b[0m     train_ds,\n\u001b[1;32m     99\u001b[0m     validation_data \u001b[39m=\u001b[39;49m valid_ds, \u001b[39m# ignored if None\u001b[39;49;00m\n\u001b[1;32m    100\u001b[0m     epochs \u001b[39m=\u001b[39;49m epochs,\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m valid_ds \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# plot losses over time --> shown after training\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:890\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    889\u001b[0m     \u001b[39m# no_variable_creation function.\u001b[39;00m\n\u001b[0;32m--> 890\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    891\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    892\u001b[0m   _, _, filtered_flat_args \u001b[39m=\u001b[39m (\n\u001b[1;32m    893\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_function_spec  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    894\u001b[0m       \u001b[39m.\u001b[39mcanonicalize_function_inputs(\n\u001b[1;32m    895\u001b[0m           args, kwds))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model1_ds = tf.data.Dataset.from_tensor_slices((email_bodies, annotation_labels))\n",
    "kfold(ds = model1_ds, epochs = 200, batch_size = 32, k = None)\n",
    "# note: I have serious memory leak problems with k-fold.\n",
    "# I'll use the following seeds to verify the average accuracy:\n",
    "# 183, 89, 11, 42, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
