{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# includes separate classifier (bottom of notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ashkan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/ashkan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2023-08-23 10:05:10.188065: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-23 10:05:10.241397: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-23 10:05:11.012502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 10:05:11.751700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 10:05:11.810923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 10:05:11.810981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# file management\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import gensim\n",
    "import gensim.downloader as gdownload\n",
    "\n",
    "# deep learning\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "print(tf.config.list_physical_devices('GPU')) # check if gpu is detected\n",
    "from keras import backend as K\n",
    "import gc\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# visualization\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "# tensorboard\n",
    "%load_ext tensorboard\n",
    "\n",
    "# performance\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email_ID</th>\n",
       "      <th>Sender</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>noreply@powerballs.com</td>\n",
       "      <td>You Have Won!</td>\n",
       "      <td>&lt;p&gt;*********PLEASE DO NOT RESPOND TO THIS EMAI...</td>\n",
       "      <td>Phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>noreply@paypalceo.com</td>\n",
       "      <td>PayPal Breach</td>\n",
       "      <td>&lt;p&gt;********* RESPONES TO THIS EMAIL WILL NOT B...</td>\n",
       "      <td>Phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>support@credit.chase.com</td>\n",
       "      <td>URGENT: Fraudulent activity detected</td>\n",
       "      <td>&lt;p&gt;Hello,&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;We are writ...</td>\n",
       "      <td>Phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>mary@yahoo.com</td>\n",
       "      <td>Donations needed for Mark</td>\n",
       "      <td>&lt;p&gt;Hello,&lt;/p&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;I&amp;#39;m contactin...</td>\n",
       "      <td>Phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>support@security.amazon.com</td>\n",
       "      <td>Your Amazon Account</td>\n",
       "      <td>&lt;p&gt;&lt;strong&gt;The account number associated with ...</td>\n",
       "      <td>Phishing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Email_ID                       Sender  \\\n",
       "0         1       noreply@powerballs.com   \n",
       "1         2        noreply@paypalceo.com   \n",
       "2         3     support@credit.chase.com   \n",
       "3         4               mary@yahoo.com   \n",
       "4         5  support@security.amazon.com   \n",
       "\n",
       "                                Subject  \\\n",
       "0                         You Have Won!   \n",
       "1                         PayPal Breach   \n",
       "2  URGENT: Fraudulent activity detected   \n",
       "3             Donations needed for Mark   \n",
       "4                  Your Amazon Account    \n",
       "\n",
       "                                               Email Email_type  \n",
       "0  <p>*********PLEASE DO NOT RESPOND TO THIS EMAI...   Phishing  \n",
       "1  <p>********* RESPONES TO THIS EMAIL WILL NOT B...   Phishing  \n",
       "2  <p>Hello,&nbsp;</p><p>&nbsp;</p><p>We are writ...   Phishing  \n",
       "3  <p>Hello,</p><p>&nbsp;</p><p>I&#39;m contactin...   Phishing  \n",
       "4  <p><strong>The account number associated with ...   Phishing  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array(['Phishing', 'Ham'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('./PhishingDataset_HFES2020.csv', encoding='windows-1252')\n",
    "df = df.replace('ham', 'Ham') # replace ham with Ham\n",
    "df = df[df['Email_type'] != 'Attention_check'] # remove Attention_check s\n",
    "display(df.head())\n",
    "display(df['Email_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 10:05:12.158683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 10:05:12.158846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 10:05:12.158888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 10:05:12.849159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 10:05:12.849267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 10:05:12.849275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-08-23 10:05:12.849307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-23 10:05:12.849354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3413 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# preprocessin funcs\n",
    "# preprocessing functions\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "def multiple_replace(arr, replace, source):\n",
    "    for item in arr:\n",
    "        source = re.sub(item, replace, source)\n",
    "\n",
    "    return source\n",
    "\n",
    "def preprocess_txt(txt):\n",
    "    set_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    # replace stuff\n",
    "    txt = re.sub(r'\\b\\S*[\\x80-\\xFF]\\S*\\b', ' ', txt) # any words with non-ascii characters\n",
    "    txt = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', ' url ', txt) # urls\n",
    "    txt = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', ' email ', txt) # emails\n",
    "    txt = re.sub(r'<.*?>', ' ', txt) # remove html tags completely\n",
    "    txt = re.sub(r'&.*?;', ' ', txt) # remove HTML entities\n",
    "    txt = re.sub(r'#', ' ', txt) # hastags --> just remove the tag\n",
    "    txt = re.sub(r'\\b\\d+\\b', ' num ', txt) # numbers\n",
    "    txt = re.sub(r'[^\\w\\s]', r' \\g<0> ', txt) # punctuation\n",
    "    \n",
    "    # lowercase\n",
    "    txt = txt.lower()\n",
    "\n",
    "    # https://saturncloud.io/blog/reshaping-text-data-for-lstm-models-in-keras-a-comprehensive-guide/\n",
    "\n",
    "    # split\n",
    "    # nltk handles all punctuation as features\n",
    "    word_arr = re.split(f'\\s+', txt) # returns list of words\n",
    "\n",
    "    # remove stopwords and drop empty strings\n",
    "    word_arr = [word for word in word_arr if word not in set_stopwords and len(word) != 0]\n",
    "    \n",
    "    # lemmatize\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    word_arr = [lemmatizer.lemmatize(word) for word in word_arr]\n",
    "\n",
    "    return word_arr\n",
    "\n",
    "def preprocess_txt_list(txt_list, embedding, sequence_length):\n",
    "\n",
    "    processed_tweets = []\n",
    "    for i, txt in enumerate(txt_list):\n",
    "\n",
    "        word_list = preprocess_txt(txt)\n",
    "        processed_tweets.append(word_list)\n",
    "                                    \n",
    "        if i % 10000 == 0: # log the processed message in specified intervals\n",
    "            print(f\"Processed text #{i}:\", word_list)\n",
    "            print(\"---------------------------\")        \n",
    "    \n",
    "    # tokenize (I ditched the old tokenizer)\n",
    "    print(\"tokenizing...\")\n",
    "    embedding_length = len(embedding)\n",
    "    # convert each word to its index. if it doesn't exist, set it to the last index. I don't care that it ruins one word's meaning\n",
    "    tokenized = [[embedding.key_to_index[word] if word in embedding else (embedding_length - 1) for word in split_sentence] for split_sentence in processed_tweets]\n",
    "\n",
    "    # add padding and convert to numpy array\n",
    "    print('padding sequences...')\n",
    "    tokenized = np.asarray(keras.preprocessing.sequence.pad_sequences(\n",
    "            tokenized,\n",
    "            padding = 'post',\n",
    "            maxlen = sequence_length,\n",
    "    ))\n",
    "\n",
    "    # DEBUG\n",
    "    print(tokenized)\n",
    "    print('feature vector shape:', tokenized.shape)\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# preprocess annotations for initial binary classification\n",
    "def preprocess_annotations(annotation_list):\n",
    "    # set all \"threat\" to 1, the rest to 0\n",
    "    return np.asarray([1 if x == \"Phishing\" else 0 for x in annotation_list])\n",
    "\n",
    "def train_valid_test_split(ds, train_ratio, valid_ratio, batch_size):\n",
    "    train_ratio = 0.8\n",
    "    valid_ratio = 0.1\n",
    "    init_len = len(ds)\n",
    "    num_train = np.floor(init_len * train_ratio)\n",
    "    num_valid = np.floor(init_len * valid_ratio)\n",
    "\n",
    "    train_ds = ds.take(num_train).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    valid_ds = ds.skip(num_train).take(num_valid).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = ds.skip(num_train).skip(num_valid).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    print(f'train ds has {num_train} items in {len(train_ds)} batches.')\n",
    "    print(f'valid ds has {num_valid} items in {len(valid_ds)} batches.')\n",
    "    print(f'test ds has {init_len - num_train - num_valid} items in {len(test_ds)} batches.')\n",
    "\n",
    "    return (train_ds, valid_ds, test_ds)\n",
    "\n",
    "def shuffle(nparr, random_state = 23):\n",
    "    rng = np.random.RandomState(random_state) # reset the seed\n",
    "    return rng.permutation(nparr)\n",
    "\n",
    "def train_and_evaluate(model, train_ds, test_ds, epochs, \n",
    "                        optimizer = keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                        loss = keras.losses.BinaryCrossentropy(),\n",
    "                        valid_ds = None):\n",
    "    model.compile(\n",
    "        loss = loss,\n",
    "        optimizer = optimizer,\n",
    "        metrics = ['acc'],\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data = valid_ds, # ignored if None\n",
    "        epochs = epochs,\n",
    "        callbacks = [tensorboard_callback],\n",
    "    )\n",
    "\n",
    "    if valid_ds != None:\n",
    "        # plot losses over time --> shown after training\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.xlabel('accuracy')\n",
    "        plt.legend(['train','val'], loc='upper left')\n",
    "        plt.grid()\n",
    "        plt.ylim(0.5, 1)\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.xlabel('loss')\n",
    "        plt.legend(['train','val'], loc='upper left')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    # evaluate\n",
    "    return (model.evaluate(test_ds)[1], model)\n",
    "\n",
    "# model training funcs + k-fold\n",
    "\n",
    "def pretrained_embedding(embedding):\n",
    "    # note: embedding is declared in the previous cell\n",
    "    \n",
    "    vocab_size = len(embedding)\n",
    "    embedding_vector_size = len(embedding[embedding.index_to_key[0]])\n",
    "\n",
    "    # create embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_vector_size))\n",
    "    # iterate through embedding and copy word vectors as weights\n",
    "    for i in range(vocab_size):\n",
    "        embedding_matrix[i, :] = embedding[embedding.index_to_key[i]]\n",
    "\n",
    "    embedding_layer = layers.Embedding(input_dim = vocab_size, output_dim = embedding_vector_size, trainable = False)\n",
    "    embedding_layer.build((None,)) # I have no idea why I should do this\n",
    "    embedding_layer.set_weights([embedding_matrix]) # square brackets are because some layers take multiple types of weights\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "def build_model(sequence_length, embedding):\n",
    "  model = keras.Sequential([\n",
    "    layers.Input(shape = (sequence_length,)),\n",
    "    pretrained_embedding(embedding),\n",
    "    # layers.GRU(MAX_TWEET_WORDS, return_sequences=True), # not a difference\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Bidirectional(\n",
    "        layers.GRU(\n",
    "            sequence_length,\n",
    "            dropout = 0.2,\n",
    "        ),\n",
    "    ),\n",
    "\n",
    "    layers.Dense(32, activation = 'relu'),\n",
    "    layers.Dropout(0.7),\n",
    "    layers.Dense(8, activation = 'relu'),\n",
    "    layers.Dropout(0.7),\n",
    "    layers.Dense(1, activation = 'sigmoid'),\n",
    "  ])\n",
    "\n",
    "  return model\n",
    "\n",
    "def kfold(ds, epochs, batch_size, k, sequence_length, embedding):\n",
    "  loss = keras.losses.BinaryCrossentropy()\n",
    "  optimizer = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "  autotune = tf.data.AUTOTUNE\n",
    "\n",
    "  if k == None:\n",
    "    # normal stuff\n",
    "    model = build_model(sequence_length, embedding)\n",
    "\n",
    "    train_ds, valid_ds, test_ds = train_valid_test_split(ds, 0.6, 0.2, batch_size)\n",
    "    train_and_evaluate(\n",
    "      model,\n",
    "      train_ds = train_ds,\n",
    "      valid_ds = valid_ds,\n",
    "      test_ds = test_ds,\n",
    "      epochs = epochs,\n",
    "      loss = loss,\n",
    "      optimizer = optimizer,\n",
    "    )\n",
    "\n",
    "  else:\n",
    "    accuracies = []\n",
    "    for i in range(k):\n",
    "      print(f'fold {i}')\n",
    "      \n",
    "      model = build_model(sequence_length, embedding)\n",
    "      num_total = len(ds)\n",
    "      num_test = np.floor(num_total / k)\n",
    "      num_train = num_total - num_test\n",
    "\n",
    "      test_range = [np.floor((i) * num_test), np.floor((i + 1) * num_test)]\n",
    "      train_ds_p1 = ds.take(test_range[0])\n",
    "      train_ds_p2 = ds.skip(test_range[1])\n",
    "      train_ds = train_ds_p1.concatenate(train_ds_p2).batch(batch_size).prefetch(autotune)\n",
    "      print(f'train dataset range: {test_range[0]} - {test_range[1]}')\n",
    "      test_ds = ds.skip(np.floor((i) * num_test)).take(num_test).batch(batch_size).prefetch(autotune)\n",
    "      print(f'test dataset range: {test_range[0]} - {test_range[1]}')\n",
    "\n",
    "      print(f'train ds has {num_train} items in {len(train_ds)} batches.')\n",
    "      print(f'test ds has {num_test} items in {len(test_ds)} batches.')\n",
    "      \n",
    "      accuracy = train_and_evaluate(\n",
    "        model,\n",
    "        train_ds,\n",
    "        test_ds,\n",
    "        epochs = epochs,\n",
    "        loss = loss,\n",
    "        optimizer = optimizer,\n",
    "      )[0]\n",
    "      \n",
    "      print(\"accuracy: \", accuracy)\n",
    "      accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"average accuracy: {np.average(accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the average length of emails\n",
    "# tweetlengths = df['Email'].apply(lambda x: len(x.split()))\n",
    "# print(np.average(tweetlengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdownload.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text #0: ['*', '*', '*', '*', '*', '*', '*', '*', '*', 'please', 'respond', 'email', '*', '*', '*', '*', '*', '*', '*', '*', '*', 'record', 'show', 'entered', 'win', 'state', 'powerball', 'jackpot', 'num', '/', 'num', '/', 'num', '.', 'receiving', 'email', 'listed', 'one', 'winner', '.', 'claim', 'prize', 'please', 'visit', 'site', 'fill', 'information', 'needed', 'collect', '.', 'must', 'process', 'information', 'within', 'week', 'time', 'may', 'lose', 'winning', '.', 'congratulation', '!', 'collect', 'earnings', '!', 'please', 'click', 'prompt', 'response', 'regarding', 'matter', 'appreciated', '.', 'sincerely', ',', 'powerball', 'team']\n",
      "---------------------------\n",
      "tokenizing...\n",
      "padding sequences...\n",
      "[[   42    42    42 ...     0     0     0]\n",
      " [   42    42    42 ...     0     0     0]\n",
      " [  996     4  3145 ...     0     0     0]\n",
      " ...\n",
      " [  589 31134     4 ...     0     0     0]\n",
      " [ 3709     1   416 ...  2471  2311     1]\n",
      " [  589     4 11532 ...     0     0     0]]\n",
      "feature vector shape: (239, 80)\n",
      "original data:\n",
      "<<0>> <p><strong>17/10/2017<br />Account No: 108-455294-800125-MN</strong></p><p>&nbsp;</p><p>To our valued <em>Walmart</em> customer,&nbsp;</p><p>&nbsp;</p><p>The team at <em>Walmart Online Services</em> is happy to announce that you have been chosen as the &#39;User of the month&#39; lottery&nbsp;winner!&nbsp;This is a monthly event where we randomly choose a customer to receive&nbsp;free Walmart coupons. In order to&nbsp;access your coupons, please visit the link below:</p><p>&nbsp;</p><p><strong><a href=\"http://walmart/rewards/coupons\" onclick=\"return false;\">Walmart Reward Coupons</a></strong></p><p>&nbsp;</p><p>Using any of these coupons will result in a donation to St. Jude&#39;s Children Hospital, so you can now receive free items and donate to a good cause at the same time. We hope you&#39;ll help support us!<br /><br /><em><strong>From,</strong></em></p><p><strong>Your Friends at Walmart Online Services</strong></p><p>&nbsp;</p>\n",
      "<<1>> <p>What would&nbsp;you do with extra money each month? &nbsp;</p><p>&nbsp;</p><p>JP Morgan&nbsp;is offering a new 30 Year Fixed Term Loan with rates starting from <em><strong>1.05%</strong></em> to qualified customers. Its easy to see if you qualify! Don&#39;t let this opportunity pass you by!</p><p>&nbsp;</p><p>To see if you qualify, please click the link below.</p><p>&nbsp;</p><p><a target=\"\" href=\"http://Chase-Security.html\" onclick=\"return false;\">https://chaseonline.com/promotional/30YearJumbo/qualification.aspx</a>&nbsp;</p><p>&nbsp;</p><p>We look forward to hearing from&nbsp;you,&nbsp;</p><p>&nbsp;</p><p>Sincerely,&nbsp;</p><p>JP Morgan Chase Bank, N.A.</p>\n",
      "<<2>> <p>Dear Loyal Customer:</p><p>&nbsp;</p><p>JP Morgan Chase Bank is excited to announce their&nbsp;30 Year Fixed Term Jumbo Loan with rates starting from 1.05% to qualified customers to refinance or get a loan for a house</p><p>&nbsp;</p><p>In only seconds you can see if you qualify, just click here. We have representives standing by.&nbsp;</p><p>&nbsp;</p><p><a target=\"\" href=\"http://Chase-Security.html\" onclick=\"return false;\">https://chaseonline.com/promotional/30YearJumbo/qualification.aspx</a>&nbsp;</p><p>&nbsp;</p><p>We look forward to serving all of your financial needs.&nbsp;</p><p>&nbsp;</p><p>Sincerely,&nbsp;</p><p>JP Morgan Chase Bank, N.A.</p><p>Chase Privacy Operations, P.O. Box 659752, San Antonio, TX 78265-9752.&nbsp;</p><p>&copy; 2016 JPMorgan Chase &amp; Co.</p>\n",
      "<<3>> <p>Dear Sir or Madam:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; We are required by law to contact you with the following information. You account is in need of prompt attention. With one click you will be able to update your account&nbsp;and continue using our simple, no risk website to continue with all your business needs. We encourage you to do this as quickly as possible. Without further action on your part, we will find it necessary to close this account.</p><p><a href=\"http://www.USSAgoogle.com\" onclick=\"return false;\">&nbsp;Please click this sentence</a></p><p>&nbsp;</p><p>Thank you for banking with us.</p><p>&nbsp;</p><p>Sincerely,</p><p>USAA Bank</p>\n",
      "<<4>> <p>To Whom It May Concern:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I possess a Master&#39;s Degree in Accounting, and am currently in the process of completing my CPA qualifications. I also have advanced experience&nbsp;in Microsoft Access, Excel, Word and Powerpoint, QuickBooks, and other accounting software. I believe that I could be a major asset to your company.</p><p>&nbsp;</p><p>Please take the time to look through my qualifications:</p><p><a target=\"_blank\" href=\"https://gmail.com/myemail\" onclick=\"return false;\">My CV</a></p><p>&nbsp;</p><p>I thank you for your time and hope that you will consider me for a position.</p><p>&nbsp;</p><p>Best,</p><p>Ryan Walters</p>\n",
      "<<5>> <p>&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; I am having some serious medical issues and could use your help cousin :(. Please donate to me on the page below and give me a call when I&#39;m all better.&nbsp;</p><p><a href=\"http://www.dfnbk.com\" onclick=\"return false;\">Link!</a></p>\n",
      "<<6>> <p>Hello,</p><p>&nbsp;</p><p>Thank you for shopping with us.</p><p>We will email you once your item&nbsp;has shipped.</p><p>&nbsp;</p><p>Order Details:</p><p>Order # 002-527657-2343090</p><p>Shipping: FREE Two-Day Shipping</p><p>Order total: $109.29</p><p>&nbsp;</p><p>If you didn&rsquo;t authorize the transaction, please <a href=\"http://www.refund.item\" onclick=\"return false;\" > Click Here </a>to receive a full refund.</p><p>&nbsp;</p><p>We hope to see you again soon.</p><p>&nbsp;</p><p>Amazon.com</p>\n",
      "<<7>> <p>&nbsp;</p><p>&nbsp;</p><p>We are writing to you because your federal Tax refund&nbsp;is now ready to be sent. To confirm your account please visit the website below.&nbsp;</p><p>If verification is not&nbsp;made within the next 72 hours your refund could substantially be delayed. Log in now to secure your refund.&nbsp;</p><p>&nbsp;</p><p>&nbsp;<a href=\"http://www.IR5.com\" onclick=\"return false;\">www.irs.com</a> &nbsp;</p><p>&nbsp;</p><p>Again, please respond within the next 72 hours to avoid delays to your refund.</p><p>&nbsp;</p><p>Sincerely,</p><p>&nbsp;</p><p>IRS Refund Department</p>\n",
      "<<8>> <p><strong>Your Account has been locked! You can resolve this now.</strong>&nbsp;</p><p>&nbsp;</p><p>Case id : 9000321-128. Login attempt from unknown device.&nbsp;</p><p>&nbsp;</p><p><strong>Dear Client&nbsp;</strong></p><p>Someone has accessed your account, so we have temporarily locked it to keep your personal informations in safe. To unlock your account, you may need to pass a security check. Note that attempting to access someone else is a violation of PayPals terms. It may also be illegal. To reset your account:&nbsp;</p><p>1-Click on the link <a target=\"\" href=\"PayPal1-Security.html\" onclick=\"return false;\">PayPal-Security.html</a>.&nbsp;</p><p>2-Open the page in a browser window secure.&nbsp;</p><p>3-Follow the instructions.</p><p>&nbsp;</p><p>&nbsp;</p>\n",
      "<<9>> <h4>Add a backup phone as an alternative 2-step Verification step</h4>andrew19@gmail.com<br><br>A backup phone number ensures that you can sign in when your main second verification step is unavailable.<br><br>Add a backup number and see other personalised recommendations in the Security Check-up.<h4><a href=\"https://www.gmail.com\" onclick=\"return false;\">TAKE ACTION </a></h4>\n",
      "------------------------------------------\n",
      "split input: \n",
      "<<0>> ['num', '/', 'num', '/', 'num', 'account', ':', 'num', '-', 'num', '-', 'num', '-', 'mn', 'valued', 'walmart', 'customer', ',', 'team', 'walmart', 'online', 'service', 'happy', 'announce', 'chosen', 'user', 'month', 'lottery', 'winner', '!', 'monthly', 'event', 'randomly', 'choose', 'customer', 'receive', 'free', 'walmart', 'coupon', '.', 'order', 'access', 'coupon', ',', 'please', 'visit', 'link', ':', 'walmart', 'reward', 'coupon', 'using', 'coupon', 'result', 'donation', 'st', '.', 'jude', 'child', 'hospital', ',', 'receive', 'free', 'item', 'donate', 'good', 'cause', 'time', '.', 'hope', 'help', 'support', 'u', '!', ',', 'friend', 'walmart', 'online', 'service', '<user>']\n",
      "<<1>> ['would', 'extra', 'money', 'month', '?', 'jp', 'morgan', 'offering', 'new', 'num', 'year', 'fixed', 'term', 'loan', 'rate', 'starting', 'num', '.', 'num', '%', 'qualified', 'customer', '.', 'easy', 'see', 'qualify', '!', 'let', 'opportunity', 'pas', '!', 'see', 'qualify', ',', 'please', 'click', 'link', '.', 'url', 'look', 'forward', 'hearing', ',', 'sincerely', ',', 'jp', 'morgan', 'chase', 'bank', ',', 'n', '.', '.', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<2>> ['dear', 'loyal', 'customer', ':', 'jp', 'morgan', 'chase', 'bank', 'excited', 'announce', 'num', 'year', 'fixed', 'term', 'jumbo', 'loan', 'rate', 'starting', 'num', '.', 'num', '%', 'qualified', 'customer', 'refinance', 'get', 'loan', 'house', 'second', 'see', 'qualify', ',', 'click', '.', 'ﾟﾟﾟｵﾔｽﾐｰ', 'standing', '.', 'url', 'look', 'forward', 'serving', 'financial', 'need', '.', 'sincerely', ',', 'jp', 'morgan', 'chase', 'bank', ',', 'n', '.', '.', 'chase', 'privacy', 'operation', ',', 'p', '.', '.', 'box', 'num', ',', 'san', 'antonio', ',', 'tx', 'num', '-', 'num', '.', 'num', 'jpmorgan', 'chase', 'co', '.', '<user>', '<user>', '<user>']\n",
      "<<3>> ['dear', 'sir', 'madam', ':', 'required', 'law', 'contact', 'following', 'information', '.', 'account', 'need', 'prompt', 'attention', '.', 'one', 'click', 'able', 'update', 'account', 'continue', 'using', 'simple', ',', 'risk', 'website', 'continue', 'business', 'need', '.', 'encourage', 'quickly', 'possible', '.', 'without', 'action', 'part', ',', 'find', 'necessary', 'close', 'account', '.', 'please', 'click', 'sentence', 'thank', 'banking', 'u', '.', 'sincerely', ',', 'usaa', 'bank', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<4>> ['may', 'concern', ':', 'posse', 'master', 'degree', 'accounting', ',', 'currently', 'process', 'completing', 'cpa', 'qualification', '.', 'also', 'advanced', 'experience', 'microsoft', 'access', ',', 'excel', ',', 'word', 'powerpoint', ',', 'quickbooks', ',', 'accounting', 'software', '.', 'believe', 'could', 'major', 'asset', 'company', '.', 'please', 'take', 'time', 'look', 'qualification', ':', 'cv', 'thank', 'time', 'hope', 'consider', 'position', '.', 'best', ',', 'ryan', 'walter', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<5>> ['serious', 'medical', 'issue', 'could', 'use', 'help', 'cousin', ':', '(', '.', 'please', 'donate', 'page', 'give', 'call', 'better', '.', 'link', '!', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<6>> ['hello', ',', 'thank', 'shopping', 'u', '.', 'email', 'item', 'shipped', '.', 'order', 'detail', ':', 'order', 'num', '-', 'num', '-', 'num', 'shipping', ':', 'free', 'two', '-', 'day', 'shipping', 'order', 'total', ':', '$', 'num', '.', 'num', 'authorize', 'transaction', ',', 'please', 'click', 'receive', 'full', 'refund', '.', 'hope', 'see', 'soon', '.', 'url', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<7>> ['writing', 'federal', 'tax', 'refund', 'ready', 'sent', '.', 'confirm', 'account', 'please', 'visit', 'website', '.', 'verification', 'made', 'within', 'next', 'num', 'hour', 'refund', 'could', 'substantially', 'delayed', '.', 'log', 'secure', 'refund', '.', 'url', ',', 'please', 'respond', 'within', 'next', 'num', 'hour', 'avoid', 'delay', 'refund', '.', 'sincerely', ',', 'irs', 'refund', 'department', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<8>> ['account', 'locked', '!', 'resolve', '.', 'case', 'id', ':', 'num', '-', 'num', '.', 'login', 'attempt', 'unknown', 'device', '.', 'dear', 'client', 'someone', 'accessed', 'account', ',', 'temporarily', 'locked', 'keep', 'personal', 'information', 'safe', '.', 'unlock', 'account', ',', 'may', 'need', 'pas', 'security', 'check', '.', 'note', 'attempting', 'access', 'someone', 'else', 'violation', 'ﾟﾟﾟｵﾔｽﾐｰ', 'term', '.', 'may', 'also', 'illegal', '.', 'reset', 'account', ':', 'num', '-', 'click', 'link', 'url', '.', 'num', '-', 'open', 'page', 'browser', 'window', 'secure', '.', 'num', '-', 'follow', 'instruction', '.', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<9>> ['add', 'backup', 'phone', 'alternative', 'num', '-', 'step', 'verification', 'step', 'url', 'backup', 'phone', 'number', 'ensures', 'sign', 'main', 'second', 'verification', 'step', 'unavailable', '.', 'add', 'backup', 'number', 'see', 'personalised', 'recommendation', 'security', 'check', '-', '.', 'take', 'action', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "------------------------------------------\n",
      "tokenized input: \n",
      "<<0>> [3709, 38, 3709, 38, 3709, 1548, 2, 3709, 28, 3709, 28, 3709, 28, 2886, 54234, 7214, 7922, 4, 302, 7214, 949, 2465, 177, 8261, 12628, 3920, 1423, 16634, 3104, 9, 21024, 2466, 8209, 2665, 7922, 6398, 424, 7214, 12285, 1, 1853, 7530, 12285, 4, 200, 1621, 1345, 2, 7214, 15037, 12285, 1802, 12285, 5678, 23473, 993, 1, 24751, 1916, 2974, 4, 6398, 424, 6657, 8223, 117, 507, 135, 1, 420, 515, 1215, 51, 9, 4, 531, 7214, 949, 2465, 0]\n",
      "<<1>> [196, 2052, 580, 1423, 14, 9182, 6711, 13830, 122, 3709, 356, 8090, 6261, 9621, 3760, 1752, 3709, 1, 3709, 299, 28111, 7922, 1, 1227, 163, 30517, 9, 265, 5571, 354, 9, 163, 30517, 4, 200, 2215, 1345, 1, 14119, 273, 1732, 3856, 4, 11404, 4, 9182, 6711, 5037, 2699, 4, 36, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<2>> [1034, 3972, 7922, 2, 9182, 6711, 5037, 2699, 870, 8261, 3709, 356, 8090, 6261, 28257, 9621, 3760, 1752, 3709, 1, 3709, 299, 28111, 7922, 158204, 87, 9621, 543, 1296, 163, 30517, 4, 2215, 1, 1193513, 4454, 1, 14119, 273, 1732, 15269, 8617, 171, 1, 11404, 4, 9182, 6711, 5037, 2699, 4, 36, 1, 1, 5037, 13958, 13787, 4, 351, 1, 1, 2271, 3709, 4, 1401, 6683, 4, 7612, 3709, 28, 3709, 1, 3709, 98039, 5037, 2384, 1, 0, 0, 0]\n",
      "<<3>> [1034, 2621, 24102, 2, 13544, 3187, 3514, 927, 5386, 1, 1548, 171, 62081, 1823, 1, 96, 2215, 1773, 1880, 1548, 3475, 1802, 1617, 4, 5889, 2529, 3475, 1249, 171, 1, 19253, 5935, 2436, 1, 663, 3196, 802, 4, 470, 10008, 1213, 1548, 1, 200, 2215, 8363, 337, 24686, 51, 1, 11404, 4, 276017, 2699, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<4>> [530, 14730, 2, 47592, 4333, 9310, 21018, 4, 3996, 7014, 51524, 38391, 78372, 1, 894, 16857, 3084, 6506, 7530, 4, 24064, 4, 892, 33943, 4, 178673, 4, 21018, 7287, 1, 552, 297, 3577, 39541, 3054, 1, 200, 284, 135, 273, 78372, 2, 9027, 337, 135, 420, 6213, 5956, 1, 209, 4, 2827, 16956, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<5>> [1507, 7294, 4649, 297, 716, 515, 2569, 2, 17, 1, 200, 8223, 1737, 374, 462, 295, 1, 1345, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<6>> [996, 4, 337, 1652, 51, 1, 2600, 6657, 27955, 1, 1853, 13794, 2, 1853, 3709, 28, 3709, 28, 3709, 7723, 2, 424, 568, 28, 125, 7723, 1853, 2413, 2, 206, 3709, 1, 3709, 284862, 80102, 4, 200, 2215, 6398, 833, 31845, 1, 420, 163, 698, 1, 14119, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<7>> [3145, 8677, 5098, 31845, 579, 2089, 1, 9255, 1548, 200, 1621, 2529, 1, 80719, 425, 4615, 411, 3709, 1194, 31845, 297, 239791, 18502, 1, 8253, 17559, 31845, 1, 14119, 4, 200, 7093, 4615, 411, 3709, 1194, 5839, 9023, 31845, 1, 11404, 4, 21039, 31845, 13272, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<8>> [1548, 7054, 9, 18309, 1, 1815, 2471, 2, 3709, 28, 3709, 1, 25037, 7990, 9697, 13249, 1, 1034, 13322, 238, 264007, 1548, 4, 30878, 7054, 416, 2615, 5386, 2184, 1, 17527, 1548, 4, 530, 171, 354, 4739, 525, 1, 2586, 22676, 7530, 238, 645, 49497, 1193513, 6261, 1, 530, 894, 7188, 1, 24401, 1548, 2, 3709, 28, 2215, 1345, 14119, 1, 3709, 28, 784, 1737, 24107, 3962, 17559, 1, 3709, 28, 84, 58103, 1, 0, 0, 0, 0, 0, 0]\n",
      "<<9>> [1894, 16464, 448, 15792, 3709, 28, 2022, 80719, 2022, 14119, 16464, 448, 1077, 148726, 1793, 1029, 1296, 80719, 2022, 60682, 1, 1894, 16464, 1077, 163, 67974, 51557, 4739, 525, 28, 1, 284, 3196, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "------------------------------------------\n",
      "labels:  [1 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# actual preprocessing\n",
    "# embedding = gdownload.load('glove-wiki-gigaword-100') # pretrained embedding --> much cleaner than twitter stuff\n",
    "embedding = gdownload.load('glove-twitter-200') # pretrained embedding \n",
    "# embedding = gdownload.load('glove-twitter-50') # pretrained embedding \n",
    "# embedding = gdownload.load('word2vec-google-news-300')\n",
    "original_texts = np.asarray(df['Email']) # I'll use this to check the preprocessing process\n",
    "\n",
    "email_bodies = preprocess_txt_list(df['Email'], embedding, sequence_length = 80) \n",
    "annotation_labels = preprocess_annotations(df['Email_type'])\n",
    "\n",
    "# shuffle the data\n",
    "\n",
    "###### vvvv SEED IS HERE vvvv ######\n",
    "# seed = 183\n",
    "# seed = 89\n",
    "# seed = 11\n",
    "# seed = 42\n",
    "seed = 30\n",
    "###### ^^^^ SEED IS HERE ^^^^ ######\n",
    "\n",
    "original_texts = shuffle(original_texts, random_state = seed) # debug\n",
    "email_bodies = shuffle(email_bodies, random_state = seed)\n",
    "annotation_labels = shuffle(annotation_labels, random_state = seed)\n",
    "# type_labels = shuffle(type_labels)\n",
    "\n",
    "# reduce data for faster training # REMOVE LATER\n",
    "ratio_keep = 1 \n",
    "original_texts = original_texts[:int(len(original_texts) * ratio_keep)] # debug\n",
    "email_bodies = email_bodies[:int(len(email_bodies) * ratio_keep)]\n",
    "annotation_labels = annotation_labels[:int(len(annotation_labels) * ratio_keep)]\n",
    "# type_labels = type_labels[:int(len(type_labels) * ratio_keep)]\n",
    "\n",
    "# DEBUG\n",
    "def print_list(title, list):\n",
    "    print(title)\n",
    "    for i, x in enumerate(list):\n",
    "        print(f'<<{i}>>', x)\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "sample_length = 10\n",
    "print_list(\"original data:\", original_texts[:sample_length])\n",
    "tokenized_input_sample = [[index for index in x] for x in email_bodies][:sample_length]\n",
    "print_list(\"split input: \", [[embedding.index_to_key[index] for index in example] for example in tokenized_input_sample])\n",
    "print_list(\"tokenized input: \", tokenized_input_sample)\n",
    "print(\"labels: \", annotation_labels[:sample_length])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ds has 191.0 items in 6 batches.\n",
      "valid ds has 23.0 items in 1 batches.\n",
      "test ds has 25.0 items in 1 batches.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 80, 200)           238702800 \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 80, 200)           800       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 160)               135360    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                5152      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 238844385 (911.12 MB)\n",
      "Trainable params: 141185 (551.50 KB)\n",
      "Non-trainable params: 238703200 (910.58 MB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 10:08:52.983983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-08-23 10:08:53.129905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-23 10:08:53.134115: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc298184510 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-23 10:08:53.134146: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-08-23 10:08:53.139077: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-23 10:08:53.241930: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-08-23 10:08:53.314790: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/6 [========================>.....] - ETA: 0s - loss: 0.6493 - acc: 0.7000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 10:09:06.852222: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (mklcpu) ran out of memory trying to allocate 53.35GiB (rounded to 57288672000)requested by op OneHot\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-08-23 10:09:06.852429: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for mklcpu\n",
      "2023-08-23 10:09:06.852439: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852443: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852448: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852450: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852453: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852456: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852459: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852461: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852464: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852466: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852469: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852471: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852474: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852476: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852479: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852481: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852484: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852487: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852489: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852494: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 2, Chunks in use: 0. 453.68MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852517: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 6, Chunks in use: 5. 8.56GiB allocated for chunks. 6.33GiB in use in bin. 6.22GiB client-requested in use in bin.\n",
      "2023-08-23 10:09:06.852522: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 53.35GiB was 256.00MiB, Chunk State: \n",
      "2023-08-23 10:09:06.852581: I tensorflow/tsl/framework/bfc_allocator.cc:1068]   Size: 2.22GiB | Requested Size: 1.78GiB | in_use: 0 | bin_num: 20, prev:   Size: 910.58MiB | Requested Size: 910.58MiB | in_use: 1 | bin_num: -1\n",
      "2023-08-23 10:09:06.852588: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 4294967296\n",
      "2023-08-23 10:09:06.852636: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7fbe47ffb040 of size 954811392 next 8\n",
      "2023-08-23 10:09:06.852641: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7fbe80e8f440 of size 954811392 next 6\n",
      "2023-08-23 10:09:06.852644: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7fbeb9d23840 of size 2385344512 next 18446744073709551615\n",
      "2023-08-23 10:09:06.852646: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 2147483648\n",
      "2023-08-23 10:09:06.852649: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7fbf4bffd040 of size 1909622528 next 4\n",
      "2023-08-23 10:09:06.852651: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7fbfbdd25740 of size 237861120 next 18446744073709551615\n",
      "2023-08-23 10:09:06.852653: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 2147483648\n",
      "2023-08-23 10:09:06.852655: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7fbfcffff040 of size 1909622528 next 2\n",
      "2023-08-23 10:09:06.852657: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7fc041d27740 of size 237861120 next 18446744073709551615\n",
      "2023-08-23 10:09:06.852659: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 1073741824\n",
      "2023-08-23 10:09:06.852662: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7fc11cfec040 of size 1073741824 next 18446744073709551615\n",
      "2023-08-23 10:09:06.852664: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-08-23 10:09:06.852667: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 954811392 totalling 1.78GiB\n",
      "2023-08-23 10:09:06.852671: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1073741824 totalling 1.00GiB\n",
      "2023-08-23 10:09:06.852673: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 1909622528 totalling 3.56GiB\n",
      "2023-08-23 10:09:06.852676: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 6.33GiB\n",
      "2023-08-23 10:09:06.852678: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 9663676416 memory_limit_: 33514868736 available bytes: 23851192320 curr_region_allocation_bytes_: 8589934592\n",
      "2023-08-23 10:09:06.852697: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                     33514868736\n",
      "InUse:                      6802609664\n",
      "MaxInUse:                   8712231936\n",
      "NumAllocs:                           8\n",
      "MaxAllocSize:               1909622528\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-08-23 10:09:06.852728: W tensorflow/tsl/framework/bfc_allocator.cc:497] ********************________________________*********************_*********************_***********x\n",
      "2023-08-23 10:09:06.852970: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at one_hot_op.cc:98 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[238702800,30] and type double on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__OneHot_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[238702800,30] and type double on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:OneHot] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model1_ds \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices((email_bodies, annotation_labels))\n\u001b[0;32m----> 2\u001b[0m kfold(\n\u001b[1;32m      3\u001b[0m   ds \u001b[39m=\u001b[39;49m model1_ds,\n\u001b[1;32m      4\u001b[0m   epochs \u001b[39m=\u001b[39;49m \u001b[39m200\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m   batch_size \u001b[39m=\u001b[39;49m \u001b[39m32\u001b[39;49m, \n\u001b[1;32m      6\u001b[0m   k \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m, \u001b[39m# anything other than non is broken\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m   sequence_length \u001b[39m=\u001b[39;49m \u001b[39mlen\u001b[39;49m(email_bodies[\u001b[39m0\u001b[39;49m]),\n\u001b[1;32m      8\u001b[0m   embedding \u001b[39m=\u001b[39;49m embedding\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[39m# note: I have serious memory leak problems with k-fold.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# I'll use the following seeds to verify the average accuracy:\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# 183, 89, 11, 42, 30\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 196\u001b[0m, in \u001b[0;36mkfold\u001b[0;34m(ds, epochs, batch_size, k, sequence_length, embedding)\u001b[0m\n\u001b[1;32m    193\u001b[0m   model \u001b[39m=\u001b[39m build_model(sequence_length, embedding)\n\u001b[1;32m    195\u001b[0m   train_ds, valid_ds, test_ds \u001b[39m=\u001b[39m train_valid_test_split(ds, \u001b[39m0.6\u001b[39m, \u001b[39m0.2\u001b[39m, batch_size)\n\u001b[0;32m--> 196\u001b[0m   train_and_evaluate(\n\u001b[1;32m    197\u001b[0m     model,\n\u001b[1;32m    198\u001b[0m     train_ds \u001b[39m=\u001b[39;49m train_ds,\n\u001b[1;32m    199\u001b[0m     valid_ds \u001b[39m=\u001b[39;49m valid_ds,\n\u001b[1;32m    200\u001b[0m     test_ds \u001b[39m=\u001b[39;49m test_ds,\n\u001b[1;32m    201\u001b[0m     epochs \u001b[39m=\u001b[39;49m epochs,\n\u001b[1;32m    202\u001b[0m     loss \u001b[39m=\u001b[39;49m loss,\n\u001b[1;32m    203\u001b[0m     optimizer \u001b[39m=\u001b[39;49m optimizer,\n\u001b[1;32m    204\u001b[0m   )\n\u001b[1;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   accuracies \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[3], line 113\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_ds, test_ds, epochs, optimizer, loss, valid_ds)\u001b[0m\n\u001b[1;32m    106\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m    107\u001b[0m     loss \u001b[39m=\u001b[39m loss,\n\u001b[1;32m    108\u001b[0m     optimizer \u001b[39m=\u001b[39m optimizer,\n\u001b[1;32m    109\u001b[0m     metrics \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    112\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39msummary())\n\u001b[0;32m--> 113\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    114\u001b[0m     train_ds,\n\u001b[1;32m    115\u001b[0m     validation_data \u001b[39m=\u001b[39;49m valid_ds, \u001b[39m# ignored if None\u001b[39;49;00m\n\u001b[1;32m    116\u001b[0m     epochs \u001b[39m=\u001b[39;49m epochs,\n\u001b[1;32m    117\u001b[0m     callbacks \u001b[39m=\u001b[39;49m [tensorboard_callback],\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m valid_ds \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[39m# plot losses over time --> shown after training\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorboard/plugins/histogram/summary_v2.py:196\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(name, data, step, buckets, description)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m@lazy_tensor_creator\u001b[39m\u001b[39m.\u001b[39mLazyTensorCreator\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlazy_tensor\u001b[39m():\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m _buckets(data, buckets)\n\u001b[0;32m--> 196\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mwrite(\n\u001b[1;32m    197\u001b[0m     tag\u001b[39m=\u001b[39mtag,\n\u001b[1;32m    198\u001b[0m     tensor\u001b[39m=\u001b[39mlazy_tensor,\n\u001b[1;32m    199\u001b[0m     step\u001b[39m=\u001b[39mstep,\n\u001b[1;32m    200\u001b[0m     metadata\u001b[39m=\u001b[39msummary_metadata,\n\u001b[1;32m    201\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorboard/util/lazy_tensor_creator.py:66\u001b[0m, in \u001b[0;36mLazyTensorCreator.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensor \u001b[39m=\u001b[39m _CALL_IN_PROGRESS_SENTINEL\n\u001b[0;32m---> 66\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensor_callable()\n\u001b[1;32m     67\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tensor\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorboard/plugins/histogram/summary_v2.py:194\u001b[0m, in \u001b[0;36mhistogram.<locals>.lazy_tensor\u001b[0;34m()\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m@lazy_tensor_creator\u001b[39m\u001b[39m.\u001b[39mLazyTensorCreator\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlazy_tensor\u001b[39m():\n\u001b[0;32m--> 194\u001b[0m     \u001b[39mreturn\u001b[39;00m _buckets(data, buckets)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorboard/plugins/histogram/summary_v2.py:293\u001b[0m, in \u001b[0;36m_buckets\u001b[0;34m(data, bucket_count)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mtranspose(a\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mstack([edges, edges, bucket_counts]))\n\u001b[1;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mcond(\n\u001b[1;32m    290\u001b[0m         has_single_value, when_single_value, when_multiple_values\n\u001b[1;32m    291\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mcond(is_empty, when_empty, when_nonempty)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorboard/plugins/histogram/summary_v2.py:289\u001b[0m, in \u001b[0;36m_buckets.<locals>.when_nonempty\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m     bucket_counts \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(\n\u001b[1;32m    284\u001b[0m         tf\u001b[39m.\u001b[39mconcat([zeroes[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], [data_size]], \u001b[39m0\u001b[39m)[:bucket_count],\n\u001b[1;32m    285\u001b[0m         dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat64,\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mtranspose(a\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mstack([edges, edges, bucket_counts]))\n\u001b[0;32m--> 289\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mcond(\n\u001b[1;32m    290\u001b[0m     has_single_value, when_single_value, when_multiple_values\n\u001b[1;32m    291\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorboard/plugins/histogram/summary_v2.py:258\u001b[0m, in \u001b[0;36m_buckets.<locals>.when_nonempty.<locals>.when_multiple_values\u001b[0;34m()\u001b[0m\n\u001b[1;32m    254\u001b[0m clamped_indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mminimum(bucket_indices, bucket_count \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    255\u001b[0m \u001b[39m# Use float64 instead of float32 to avoid accumulating floating point error\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m# later in tf.reduce_sum when summing more than 2^24 individual `1.0` values.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# See https://github.com/tensorflow/tensorflow/issues/51419 for details.\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m one_hots \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mone_hot(\n\u001b[1;32m    259\u001b[0m     clamped_indices, depth\u001b[39m=\u001b[39;49mbucket_count, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mfloat64\n\u001b[1;32m    260\u001b[0m )\n\u001b[1;32m    261\u001b[0m bucket_counts \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(\n\u001b[1;32m    262\u001b[0m     tf\u001b[39m.\u001b[39mreduce_sum(input_tensor\u001b[39m=\u001b[39mone_hots, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[1;32m    263\u001b[0m     dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat64,\n\u001b[1;32m    264\u001b[0m )\n\u001b[1;32m    265\u001b[0m edges \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlinspace(min_, max_, bucket_count \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__OneHot_device_/job:localhost/replica:0/task:0/device:CPU:0}} OOM when allocating tensor with shape[238702800,30] and type double on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu [Op:OneHot] name: "
     ]
    }
   ],
   "source": [
    "model1_ds = tf.data.Dataset.from_tensor_slices((email_bodies, annotation_labels))\n",
    "kfold(\n",
    "  ds = model1_ds,\n",
    "  epochs = 200,\n",
    "  batch_size = 32, \n",
    "  k = None, # anything other than non is broken\n",
    "  sequence_length = len(email_bodies[0]),\n",
    "  embedding = embedding\n",
    ")\n",
    "# note: I have serious memory leak problems with k-fold.\n",
    "# I'll use the following seeds to verify the average accuracy:\n",
    "# 183, 89, 11, 42, 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ds has 191.0 items in 6 batches.\n",
      "valid ds has 23.0 items in 1 batches.\n",
      "test ds has 25.0 items in 1 batches.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 80, 200)           238702800 \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 80, 200)           800       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 160)               135360    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                5152      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 264       \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 238844385 (911.12 MB)\n",
      "Trainable params: 141185 (551.50 KB)\n",
      "Non-trainable params: 238703200 (910.58 MB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 10:03:31.910959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-08-23 10:03:32.139663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-08-23 10:03:32.147404: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f53ec0217f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-08-23 10:03:32.147456: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-08-23 10:03:32.174796: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-08-23 10:03:32.392614: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-08-23 10:03:32.533617: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 6s 35ms/step - loss: 0.7958 - acc: 0.6545\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 24ms/step - loss: 0.6686 - acc: 0.7435\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.6188 - acc: 0.7487\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.6499 - acc: 0.7749\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.6033 - acc: 0.7644\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.5525 - acc: 0.7644\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.6024 - acc: 0.7592\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.5466 - acc: 0.7330\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.6388 - acc: 0.7435\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.5728 - acc: 0.7592\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5889 - acc: 0.7644\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.5976 - acc: 0.7853\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5497 - acc: 0.7539\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.5340 - acc: 0.7696\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.5178 - acc: 0.7592\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.5925 - acc: 0.7696\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4887 - acc: 0.7958\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.4796 - acc: 0.7696\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.5574 - acc: 0.7801\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.5465 - acc: 0.7435\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.5068 - acc: 0.7906\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.4685 - acc: 0.7801\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.5282 - acc: 0.7801\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4617 - acc: 0.7801\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4727 - acc: 0.7958\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4393 - acc: 0.7592\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4631 - acc: 0.7801\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4402 - acc: 0.8010\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4889 - acc: 0.7749\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4218 - acc: 0.7801\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4425 - acc: 0.8010\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.4719 - acc: 0.7906\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.4607 - acc: 0.8010\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.4003 - acc: 0.8115\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3994 - acc: 0.8168\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.3944 - acc: 0.7853\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4124 - acc: 0.8010\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3969 - acc: 0.7906\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.4003 - acc: 0.8010\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3961 - acc: 0.8168\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.3806 - acc: 0.7853\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.3266 - acc: 0.8168\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3543 - acc: 0.8220\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3001 - acc: 0.8272\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3398 - acc: 0.8115\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.3690 - acc: 0.8063\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2919 - acc: 0.8272\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3516 - acc: 0.7958\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.3760 - acc: 0.7801\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.3119 - acc: 0.8010\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3166 - acc: 0.8010\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.3390 - acc: 0.8220\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2905 - acc: 0.8010\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2738 - acc: 0.8220\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.2497 - acc: 0.8168\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2883 - acc: 0.8115\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.2788 - acc: 0.8377\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.2870 - acc: 0.8010\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 22ms/step - loss: 0.2648 - acc: 0.8272\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2854 - acc: 0.8272\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2737 - acc: 0.8115\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2590 - acc: 0.8168\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2635 - acc: 0.8220\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2600 - acc: 0.8220\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2396 - acc: 0.8482\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2797 - acc: 0.8325\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.3082 - acc: 0.7906\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2731 - acc: 0.8377\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2248 - acc: 0.8272\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2705 - acc: 0.8115\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2434 - acc: 0.8115\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2574 - acc: 0.8168\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2432 - acc: 0.8325\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.1796 - acc: 0.8796\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2459 - acc: 0.8377\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.2029 - acc: 0.8796\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2635 - acc: 0.8220\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 17ms/step - loss: 0.2405 - acc: 0.8377\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2059 - acc: 0.8639\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2200 - acc: 0.8377\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2270 - acc: 0.8272\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2249 - acc: 0.8377\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2012 - acc: 0.8586\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2277 - acc: 0.8272\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2337 - acc: 0.8586\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2216 - acc: 0.8325\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2637 - acc: 0.8272\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2199 - acc: 0.8639\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2211 - acc: 0.8482\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1892 - acc: 0.8534\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 18ms/step - loss: 0.2126 - acc: 0.8639\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1941 - acc: 0.8691\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.2136 - acc: 0.8272\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2067 - acc: 0.8377\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2096 - acc: 0.8743\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 19ms/step - loss: 0.2313 - acc: 0.8639\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1825 - acc: 0.8639\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.2034 - acc: 0.9005\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 20ms/step - loss: 0.1701 - acc: 0.8796\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 21ms/step - loss: 0.1979 - acc: 0.8691\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 0.1034 - acc: 0.9200\n"
     ]
    }
   ],
   "source": [
    "# train model for live classification\n",
    "\n",
    "# live_model_ds = tf.data.Dataset.from_tensor_slices((email_bodies, annotation_labels))\n",
    "# live_model_train, _, live_model_test = train_valid_test_split(live_model_ds, 0.8, 0, batch_size = 32)\n",
    "\n",
    "# sequence_length = len(email_bodies[0])\n",
    "# live_model = build_model(sequence_length, embedding)\n",
    "# _, live_model = train_and_evaluate(live_model, live_model_train, live_model_test, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text #0: ['.', '.', '.']\n",
      "---------------------------\n",
      "tokenizing...\n",
      "padding sequences...\n",
      "[[1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0]]\n",
      "feature vector shape: (1, 80)\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[[0.9895074]]\n",
      "Phishing\n"
     ]
    }
   ],
   "source": [
    "# live classification\n",
    "input = \"\"\n",
    "word_arr = preprocess_txt_list([input], embedding, sequence_length)\n",
    "\n",
    "prediction = live_model.predict(word_arr)\n",
    "print(prediction)\n",
    "prediction = 1 if prediction > 0.5 else 0\n",
    "\n",
    "prediction_to_class = {\n",
    "  0: \"Ham\",\n",
    "  1: \"Phishing\"\n",
    "}\n",
    "\n",
    "print(prediction_to_class[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
