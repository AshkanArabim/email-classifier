{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-07 05:18:22.935962: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-07 05:18:23.735669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-07 05:18:24.382879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-07 05:18:24.405051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-09-07 05:18:24.405279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import gensim.downloader as gdownload\n",
    "\n",
    "# deep learning\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "print(tf.config.list_physical_devices('GPU')) # check if gpu is detected\n",
    "from keras import backend as K\n",
    "\n",
    "# visualization\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessin funcs\n",
    "# preprocessing functions\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "def multiple_replace(arr, replace, source):\n",
    "    for item in arr:\n",
    "        source = re.sub(item, replace, source)\n",
    "\n",
    "    return source\n",
    "\n",
    "\n",
    "def preprocess_txt(txt):\n",
    "    set_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    # replace stuff\n",
    "    # any words with non-ascii characters\n",
    "    txt = re.sub(r'\\b\\S*[\\x80-\\xFF]\\S*\\b', ' ', txt)\n",
    "    txt = re.sub(\n",
    "        r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', ' url ', txt)  # urls\n",
    "    txt = re.sub(\n",
    "        r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', ' email ', txt)  # emails\n",
    "    txt = re.sub(r'<.*?>', ' ', txt)  # remove html tags completely\n",
    "    txt = re.sub(r'&.*?;', ' ', txt)  # remove HTML entities\n",
    "    txt = re.sub(r'#', ' ', txt)  # hastags --> just remove the tag\n",
    "    txt = re.sub(r'\\b\\d+\\b', ' num ', txt)  # numbers\n",
    "    txt = re.sub(r'[^\\w\\s]', r' \\g<0> ', txt)  # punctuation\n",
    "\n",
    "    # lowercase\n",
    "    txt = txt.lower()\n",
    "\n",
    "    # https://saturncloud.io/blog/reshaping-text-data-for-lstm-models-in-keras-a-comprehensive-guide/\n",
    "\n",
    "    # split\n",
    "    # nltk handles all punctuation as features\n",
    "    word_arr = re.split(f'\\s+', txt)  # returns list of words\n",
    "\n",
    "    # remove stopwords and drop empty strings\n",
    "    word_arr = [\n",
    "        word for word in word_arr if word not in set_stopwords and len(word) != 0]\n",
    "\n",
    "    # lemmatize\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    word_arr = [lemmatizer.lemmatize(word) for word in word_arr]\n",
    "\n",
    "    return word_arr\n",
    "\n",
    "def dataset_to_numpy(df, colnames):\n",
    "    df_dropped = df.drop([x for x in df.columns if x not in colnames], axis = 1)\n",
    "    return df_dropped.to_numpy()\n",
    "\n",
    "def preprocess_txt_list(txt_list, embedding, sequence_length):\n",
    "\n",
    "    processed_tweets = []\n",
    "    for i, txt in enumerate(txt_list):\n",
    "\n",
    "        # DEBUG\n",
    "        try:\n",
    "            _ = txt.lower()\n",
    "        except:\n",
    "            \"Empty string found\"\n",
    "            txt = \"\"\n",
    "\n",
    "        word_list = preprocess_txt(txt)\n",
    "        processed_tweets.append(word_list)\n",
    "\n",
    "        if i % 10000 == 0:  # log the processed message in specified intervals\n",
    "            print(f\"Processed text #{i}:\", word_list)\n",
    "            print(\"---------------------------\")\n",
    "\n",
    "    # tokenize (I ditched the old tokenizer)\n",
    "    print(\"tokenizing...\")\n",
    "    embedding_length = len(embedding)\n",
    "    # convert each word to its index. if it doesn't exist, set it to the last index. I don't care that it ruins one word's meaning\n",
    "    tokenized = [[embedding.key_to_index[word] if word in embedding else (\n",
    "        embedding_length - 1) for word in split_sentence] for split_sentence in processed_tweets]\n",
    "\n",
    "    # add padding and convert to numpy array\n",
    "    print('padding sequences...')\n",
    "    tokenized = np.asarray(keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized,\n",
    "        padding='post',\n",
    "        maxlen=sequence_length,\n",
    "    ))\n",
    "\n",
    "    # DEBUG\n",
    "    print(tokenized)\n",
    "    print('feature vector shape:', tokenized.shape)\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "# preprocess annotations for initial binary classification\n",
    "\n",
    "def dataset_to_numpy(df, colnames):\n",
    "    df_dropped = df.drop([x for x in df.columns if x not in colnames], axis = 1)\n",
    "    return df_dropped.to_numpy()\n",
    "\n",
    "def preprocess_annotations(annotation_list):\n",
    "    # set all \"threat\" to 1, the rest to 0\n",
    "    return np.asarray([1 if x in [\"Phishing\", \"Phishing Email\"] else 0 for x in annotation_list])\n",
    "\n",
    "# note: this takes already shuffled data\n",
    "def train_valid_test_split(ds, train_ratio, valid_ratio, batch_size):\n",
    "    if type(ds) is list: # if array, assuming they're in array / np array format\n",
    "\n",
    "        print(type(ds), len(ds)) # DEBUG\n",
    "\n",
    "        # just to be sure they're numpy...\n",
    "        for i in range(len(ds)):\n",
    "            ds[i] = np.asarray(ds[i])\n",
    "\n",
    "        init_len = len(ds[0])\n",
    "        num_train = int(init_len * train_ratio)\n",
    "        num_valid = int(init_len * valid_ratio)\n",
    "\n",
    "        print(type(ds), len(ds)) # DEBUG\n",
    "\n",
    "        train_ds = [sublist[:num_train] for sublist in ds]\n",
    "        #     ds[0][:num_train],\n",
    "        #     ds[1][:num_train]\n",
    "        # ]\n",
    "\n",
    "        print(type(train_ds), len(train_ds)) # DEBUG\n",
    "\n",
    "        valid_ds = [sublist[num_train : num_train + num_valid] for sublist in ds]\n",
    "        #     ds[0][num_train : num_train + num_valid],\n",
    "        #     ds[1][num_train : num_train + num_valid]\n",
    "        # ]\n",
    "\n",
    "        print(type(valid_ds), len(valid_ds)) # DEBUG\n",
    "\n",
    "        test_ds = [sublist[num_train + num_valid :] for sublist in ds]\n",
    "        #     ds[0][num_train + num_valid :],\n",
    "        #     ds[1][num_train + num_valid :]\n",
    "        # ]\n",
    "\n",
    "        print(type(test_ds), len(test_ds)) # DEBUG\n",
    "\n",
    "        print(f'train ds has {len(train_ds[0])} items.')\n",
    "        print(f'valid ds has {len(valid_ds[0])} items.')\n",
    "        print(f'test ds has {len(test_ds[0])} items.')\n",
    "        \n",
    "    else: # if not array, we assume it's in dataset format\n",
    "        init_len = len(ds)\n",
    "        num_train = int(init_len * train_ratio)\n",
    "        num_valid = int(init_len * valid_ratio)\n",
    "\n",
    "        train_ds = ds.take(num_train).batch(batch_size)\n",
    "        valid_ds = ds.skip(num_train).take(num_valid).batch(batch_size)\n",
    "        test_ds = ds.skip(num_train).skip(num_valid).batch(batch_size)\n",
    "\n",
    "        print(f'train ds has {len(train_ds)} items.')\n",
    "        print(f'valid ds has {len(valid_ds)} items.')\n",
    "        print(f'test ds has {len(test_ds)} items.')\n",
    "\n",
    "    return (train_ds, valid_ds, test_ds)\n",
    "\n",
    "def shuffle(nparr, random_state=23):\n",
    "    rng = np.random.RandomState(random_state)  # reset the seed\n",
    "    return rng.permutation(nparr)\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_ds, test_ds, epochs, batch_size, valid_ds=None):\n",
    "    print(model.summary())\n",
    "\n",
    "    if type(train_ds) is tuple:\n",
    "        history = model.fit(\n",
    "            train_ds[0], # x: \n",
    "            train_ds[1], # y\n",
    "            validation_data = valid_ds, # tuples are allowed, ignored if none\n",
    "            epochs = epochs,\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "    else:\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=valid_ds,  # ignored if None\n",
    "            epochs=epochs,\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "    if valid_ds != None:\n",
    "        # plot losses over time --> shown after training\n",
    "        plt.plot(history.history['acc'])\n",
    "        plt.plot(history.history['val_acc'])\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.xlabel('accuracy')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.grid()\n",
    "        plt.ylim(0.5, 1)\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.xlabel('loss')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(history.history['auc'])\n",
    "        plt.plot(history.history['val_auc'])\n",
    "        plt.title('PR AUC')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.xlabel('AUC')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "    # evaluate\n",
    "    evaluation = model.evaluate(\n",
    "        test_ds[0], # x\n",
    "        test_ds[1], # y\n",
    "        batch_size = batch_size,\n",
    "    )\n",
    "    # predictions = model.predict(\n",
    "    #     test_ds[0], # x\n",
    "    # )\n",
    "    # accuracy = evaluation[1]\n",
    "    # print(type(predictions)) # DEBUG\n",
    "    # print(np.asarray(predictions)) # DEBUG\n",
    "    # # predictions = [0 if x < 0.5 else 1 for x in predictions]\n",
    "    # predictions = np.asarray(predictions)\n",
    "    # predictions = binary_thresholding(predictions)\n",
    "    # # test_labels = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "    # test_labels = test_ds[1]\n",
    "\n",
    "    # print(\"actual labels: \", test_labels)\n",
    "    # print(\"predicted labels: \", predictions)\n",
    "    # print(evaluation)\n",
    "    # conf_matrix = np.asarray(\n",
    "    #     tf.math.confusion_matrix(test_labels, predictions))\n",
    "\n",
    "    # print(\"confusion matrix:\")\n",
    "    # print(conf_matrix)\n",
    "\n",
    "    # conf_matrix_norm = np.asarray(\n",
    "    #     [conf_matrix[idx] / (np.sum(row) + 1e-7) for idx, row in enumerate(conf_matrix)])\n",
    "    # print('confusion matrix (percentage):')\n",
    "    # print(conf_matrix_norm)\n",
    "\n",
    "    return (\n",
    "        evaluation[1], # accuracy\n",
    "        model\n",
    "    )\n",
    "\n",
    "# model training funcs + k-fold\n",
    "\n",
    "\n",
    "def pretrained_embedding(embedding):\n",
    "    # note: embedding is declared in the previous cell\n",
    "\n",
    "    vocab_size = len(embedding)\n",
    "    embedding_vector_size = len(embedding[embedding.index_to_key[0]])\n",
    "\n",
    "    # create embedding matrix\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_vector_size))\n",
    "    # iterate through embedding and copy word vectors as weights\n",
    "    for i in range(vocab_size):\n",
    "        embedding_matrix[i, :] = embedding[embedding.index_to_key[i]]\n",
    "\n",
    "    embedding_layer = layers.Embedding(\n",
    "        input_dim=vocab_size, output_dim=embedding_vector_size, trainable=False)\n",
    "    embedding_layer.build((None,))  # I have no idea why I should do this\n",
    "    # square brackets are because some layers take multiple types of weights\n",
    "    embedding_layer.set_weights([embedding_matrix])\n",
    "\n",
    "    return embedding_layer\n",
    "\n",
    "def binary_thresholding(element, threshold = 0.5):\n",
    "    return 1 if element >= threshold else 0\n",
    "\n",
    "# def kfold(ds, epochs, batch_size, k, sequence_length, embedding):\n",
    "#     autotune = tf.data.AUTOTUNE\n",
    "\n",
    "#     if k == None:\n",
    "#         # normal stuff\n",
    "#         model = build_model(sequence_length, embedding)\n",
    "\n",
    "#         train_ds, valid_ds, test_ds = train_valid_test_split(\n",
    "#             ds, 0.6, 0.2, batch_size)\n",
    "#         train_and_evaluate(\n",
    "#             model,\n",
    "#             train_ds=train_ds,\n",
    "#             valid_ds=valid_ds,\n",
    "#             test_ds=test_ds,\n",
    "#             epochs=epochs,\n",
    "#         )\n",
    "\n",
    "#     else:\n",
    "#         accuracies = []\n",
    "#         for i in range(k):\n",
    "#             print(f'fold {i}')\n",
    "\n",
    "#             model = build_model(sequence_length, embedding)\n",
    "#             num_total = len(ds)\n",
    "#             num_test = np.floor(num_total / k)\n",
    "#             num_train = num_total - num_test\n",
    "\n",
    "#             test_range = [np.floor((i) * num_test),\n",
    "#                           np.floor((i + 1) * num_test)]\n",
    "#             train_ds_p1 = ds.take(test_range[0])\n",
    "#             train_ds_p2 = ds.skip(test_range[1])\n",
    "#             train_ds = train_ds_p1.concatenate(\n",
    "#                 train_ds_p2).batch(batch_size).prefetch(autotune)\n",
    "#             print(f'train dataset range: {test_range[0]} - {test_range[1]}')\n",
    "#             test_ds = ds.skip(\n",
    "#                 np.floor((i) * num_test)).take(num_test).batch(batch_size).prefetch(autotune)\n",
    "#             print(f'test dataset range: {test_range[0]} - {test_range[1]}')\n",
    "\n",
    "#             print(\n",
    "#                 f'train ds has {num_train} items in {len(train_ds)} batches.')\n",
    "#             print(f'test ds has {num_test} items in {len(test_ds)} batches.')\n",
    "\n",
    "#             accuracy = train_and_evaluate(\n",
    "#                 model,\n",
    "#                 train_ds,\n",
    "#                 test_ds,\n",
    "#                 epochs=epochs,\n",
    "#             )[0]\n",
    "\n",
    "#             print(\"accuracy: \", accuracy)\n",
    "#             accuracies.append(accuracy)\n",
    "\n",
    "#         print(f\"average accuracy: {np.average(accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Email_ID</th>\n",
       "      <th>Sender</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Email</th>\n",
       "      <th>Email_type</th>\n",
       "      <th>sender_mismatch</th>\n",
       "      <th>request_credentials</th>\n",
       "      <th>subject_suspicious</th>\n",
       "      <th>urgent</th>\n",
       "      <th>offer</th>\n",
       "      <th>Link_Mismatch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>235</td>\n",
       "      <td>no_reply@snapchat.com</td>\n",
       "      <td>Snapchat Login on February 15, 2019</td>\n",
       "      <td>&lt;h3&gt;Just logged in! &lt;/h3&gt;&lt;br&gt;Hi Nancy95,&lt;p&gt;It ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>236</td>\n",
       "      <td>security_awareness@secarmour.com</td>\n",
       "      <td>January Meeting: 2019</td>\n",
       "      <td>Hi Olivia,&lt;br&gt;&lt;h3 align=\"center\"&gt;Deep Dive: 20...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>237</td>\n",
       "      <td>no-reply@yahoo.com</td>\n",
       "      <td>Password change for your Yahoo account</td>\n",
       "      <td>Hi Ethan,&lt;br&gt;&lt;br&gt;The password for your Yahoo a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>238</td>\n",
       "      <td>communications@em.aetna.com</td>\n",
       "      <td>Protect your health records on your Aetna memb...</td>\n",
       "      <td>&lt;h2 align=\"center\"&gt;Protecting your personal in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>239</td>\n",
       "      <td>no-reply@dropboxmail.com</td>\n",
       "      <td>jacab invited you to check out Dropbox</td>\n",
       "      <td>Hi there,&lt;br&gt;&lt;p&gt;Jacob (jacob14@gmail.com) thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Email_ID                            Sender  \\\n",
       "234       235             no_reply@snapchat.com   \n",
       "235       236  security_awareness@secarmour.com   \n",
       "236       237                no-reply@yahoo.com   \n",
       "237       238       communications@em.aetna.com   \n",
       "238       239          no-reply@dropboxmail.com   \n",
       "\n",
       "                                               Subject  \\\n",
       "234                Snapchat Login on February 15, 2019   \n",
       "235                              January Meeting: 2019   \n",
       "236             Password change for your Yahoo account   \n",
       "237  Protect your health records on your Aetna memb...   \n",
       "238             jacab invited you to check out Dropbox   \n",
       "\n",
       "                                                 Email  Email_type  \\\n",
       "234  <h3>Just logged in! </h3><br>Hi Nancy95,<p>It ...           0   \n",
       "235  Hi Olivia,<br><h3 align=\"center\">Deep Dive: 20...           0   \n",
       "236  Hi Ethan,<br><br>The password for your Yahoo a...           0   \n",
       "237  <h2 align=\"center\">Protecting your personal in...           0   \n",
       "238  Hi there,<br><p>Jacob (jacob14@gmail.com) thin...           0   \n",
       "\n",
       "     sender_mismatch  request_credentials  subject_suspicious  urgent  offer  \\\n",
       "234                0                    0                   0       1      0   \n",
       "235                0                    0                   0       0      0   \n",
       "236                0                    0                   0       0      0   \n",
       "237                0                    0                   0       0      0   \n",
       "238                0                    0                   0       0      0   \n",
       "\n",
       "     Link_Mismatch  \n",
       "234              0  \n",
       "235              0  \n",
       "236              0  \n",
       "237              0  \n",
       "238              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average title length: 4.815899581589958\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv('../data/Emails_With_Cues.csv', encoding = \"windows-1254\")\n",
    "df = df.rename(columns = {'subject_suspecious': 'subject_suspicious'})\n",
    "df = df.replace(['Phishing', 'phishing', 'Ham', 'ham', 'Attention_check'], [1, 1, 0, 0, -1])\n",
    "df = df[df['Email_type'] != -1]\n",
    "display(df.tail())\n",
    "display(df['Email_type'].unique())\n",
    "print(\"average title length:\", np.average([len(x.split()) for x in df['Subject']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 1, 1],\n",
       "       [1, 0, 1, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_test = df[180 : 190]\n",
    "test_numpy_input = dataset_to_numpy(df_test, ['Email_type', 'sender_mismatch', 'request_credentials', 'subject_suspicious', 'urgent', 'offer', 'Link_Mismatch'])\n",
    "display(test_numpy_input)\n",
    "\n",
    "# test_numpy_input = np.insert([3], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed text #0: []\n",
      "---------------------------\n",
      "tokenizing...\n",
      "padding sequences...\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "feature vector shape: (239, 8)\n",
      "Processed text #0: ['*', '*', '*', '*', '*', '*', '*', '*', '*', 'please', 'respond', 'email', '*', '*', '*', '*', '*', '*', '*', '*', '*', 'record', 'show', 'entered', 'win', 'state', 'powerball', 'jackpot', 'num', '/', 'num', '/', 'num', '.', 'receiving', 'email', 'listed', 'one', 'winner', '.', 'claim', 'prize', 'please', 'visit', 'site', 'fill', 'information', 'needed', 'collect', '.', 'must', 'process', 'information', 'within', 'week', 'time', 'may', 'lose', 'winning', '.', 'congratulation', '!', 'collect', 'earnings', '!', 'please', 'click', 'prompt', 'response', 'regarding', 'matter', 'appreciated', '.', 'sincerely', ',', 'powerball', 'team']\n",
      "---------------------------\n",
      "tokenizing...\n",
      "padding sequences...\n",
      "[[   42    42    42 ...     0     0     0]\n",
      " [   42    42    42 ...     0     0     0]\n",
      " [  996     4  3145 ...     0     0     0]\n",
      " ...\n",
      " [  589 31134     4 ...     0     0     0]\n",
      " [ 3709     1   416 ...  2471  2311     1]\n",
      " [  589     4 11532 ...     0     0     0]]\n",
      "feature vector shape: (239, 80)\n",
      "original data:\n",
      "<<0>> <p><strong>17/10/2017<br />Account No: 108-455294-800125-MN</strong></p><p>&nbsp;</p><p>To our valued <em>Walmart</em> customer,&nbsp;</p><p>&nbsp;</p><p>The team at <em>Walmart Online Services</em> is happy to announce that you have been chosen as the &#39;User of the month&#39; lottery&nbsp;winner!&nbsp;This is a monthly event where we randomly choose a customer to receive&nbsp;free Walmart coupons. In order to&nbsp;access your coupons, please visit the link below:</p><p>&nbsp;</p><p><strong><a href=\"http://walmart/rewards/coupons\" onclick=\"return false;\">Walmart Reward Coupons</a></strong></p><p>&nbsp;</p><p>Using any of these coupons will result in a donation to St. Jude&#39;s Children Hospital, so you can now receive free items and donate to a good cause at the same time. We hope you&#39;ll help support us!<br /><br /><em><strong>From,</strong></em></p><p><strong>Your Friends at Walmart Online Services</strong></p><p>&nbsp;</p>\n",
      "<<1>> <p>What would&nbsp;you do with extra money each month? &nbsp;</p><p>&nbsp;</p><p>JP Morgan&nbsp;is offering a new 30 Year Fixed Term Loan with rates starting from <em><strong>1.05%</strong></em> to qualified customers. Its easy to see if you qualify! Don&#39;t let this opportunity pass you by!</p><p>&nbsp;</p><p>To see if you qualify, please click the link below.</p><p>&nbsp;</p><p><a target=\"\" href=\"http://Chase-Security.html\" onclick=\"return false;\">https://chaseonline.com/promotional/30YearJumbo/qualification.aspx</a>&nbsp;</p><p>&nbsp;</p><p>We look forward to hearing from&nbsp;you,&nbsp;</p><p>&nbsp;</p><p>Sincerely,&nbsp;</p><p>JP Morgan Chase Bank, N.A.</p>\n",
      "<<2>> <p>Dear Loyal Customer:</p><p>&nbsp;</p><p>JP Morgan Chase Bank is excited to announce their&nbsp;30 Year Fixed Term Jumbo Loan with rates starting from 1.05% to qualified customers to refinance or get a loan for a house</p><p>&nbsp;</p><p>In only seconds you can see if you qualify, just click here. We have representives standing by.&nbsp;</p><p>&nbsp;</p><p><a target=\"\" href=\"http://Chase-Security.html\" onclick=\"return false;\">https://chaseonline.com/promotional/30YearJumbo/qualification.aspx</a>&nbsp;</p><p>&nbsp;</p><p>We look forward to serving all of your financial needs.&nbsp;</p><p>&nbsp;</p><p>Sincerely,&nbsp;</p><p>JP Morgan Chase Bank, N.A.</p><p>Chase Privacy Operations, P.O. Box 659752, San Antonio, TX 78265-9752.&nbsp;</p><p>&copy; 2016 JPMorgan Chase &amp; Co.</p>\n",
      "<<3>> <p>Dear Sir or Madam:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; We are required by law to contact you with the following information. You account is in need of prompt attention. With one click you will be able to update your account&nbsp;and continue using our simple, no risk website to continue with all your business needs. We encourage you to do this as quickly as possible. Without further action on your part, we will find it necessary to close this account.</p><p><a href=\"http://www.USSAgoogle.com\" onclick=\"return false;\">&nbsp;Please click this sentence</a></p><p>&nbsp;</p><p>Thank you for banking with us.</p><p>&nbsp;</p><p>Sincerely,</p><p>USAA Bank</p>\n",
      "<<4>> <p>To Whom It May Concern:</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I possess a Master&#39;s Degree in Accounting, and am currently in the process of completing my CPA qualifications. I also have advanced experience&nbsp;in Microsoft Access, Excel, Word and Powerpoint, QuickBooks, and other accounting software. I believe that I could be a major asset to your company.</p><p>&nbsp;</p><p>Please take the time to look through my qualifications:</p><p><a target=\"_blank\" href=\"https://gmail.com/myemail\" onclick=\"return false;\">My CV</a></p><p>&nbsp;</p><p>I thank you for your time and hope that you will consider me for a position.</p><p>&nbsp;</p><p>Best,</p><p>Ryan Walters</p>\n",
      "<<5>> <p>&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; I am having some serious medical issues and could use your help cousin :(. Please donate to me on the page below and give me a call when I&#39;m all better.&nbsp;</p><p><a href=\"http://www.dfnbk.com\" onclick=\"return false;\">Link!</a></p>\n",
      "<<6>> <p>Hello,</p><p>&nbsp;</p><p>Thank you for shopping with us.</p><p>We will email you once your item&nbsp;has shipped.</p><p>&nbsp;</p><p>Order Details:</p><p>Order # 002-527657-2343090</p><p>Shipping: FREE Two-Day Shipping</p><p>Order total: $109.29</p><p>&nbsp;</p><p>If you didn&rsquo;t authorize the transaction, please <a href=\"http://www.refund.item\" onclick=\"return false;\" > Click Here </a>to receive a full refund.</p><p>&nbsp;</p><p>We hope to see you again soon.</p><p>&nbsp;</p><p>Amazon.com</p>\n",
      "<<7>> <p>&nbsp;</p><p>&nbsp;</p><p>We are writing to you because your federal Tax refund&nbsp;is now ready to be sent. To confirm your account please visit the website below.&nbsp;</p><p>If verification is not&nbsp;made within the next 72 hours your refund could substantially be delayed. Log in now to secure your refund.&nbsp;</p><p>&nbsp;</p><p>&nbsp;<a href=\"http://www.IR5.com\" onclick=\"return false;\">www.irs.com</a> &nbsp;</p><p>&nbsp;</p><p>Again, please respond within the next 72 hours to avoid delays to your refund.</p><p>&nbsp;</p><p>Sincerely,</p><p>&nbsp;</p><p>IRS Refund Department</p>\n",
      "<<8>> <p><strong>Your Account has been locked! You can resolve this now.</strong>&nbsp;</p><p>&nbsp;</p><p>Case id : 9000321-128. Login attempt from unknown device.&nbsp;</p><p>&nbsp;</p><p><strong>Dear Client&nbsp;</strong></p><p>Someone has accessed your account, so we have temporarily locked it to keep your personal informations in safe. To unlock your account, you may need to pass a security check. Note that attempting to access someone else is a violation of PayPals terms. It may also be illegal. To reset your account:&nbsp;</p><p>1-Click on the link <a target=\"\" href=\"PayPal1-Security.html\" onclick=\"return false;\">PayPal-Security.html</a>.&nbsp;</p><p>2-Open the page in a browser window secure.&nbsp;</p><p>3-Follow the instructions.</p><p>&nbsp;</p><p>&nbsp;</p>\n",
      "<<9>> <h4>Add a backup phone as an alternative 2-step Verification step</h4>andrew19@gmail.com<br><br>A backup phone number ensures that you can sign in when your main second verification step is unavailable.<br><br>Add a backup number and see other personalised recommendations in the Security Check-up.<h4><a href=\"https://www.gmail.com\" onclick=\"return false;\">TAKE ACTION </a></h4>\n",
      "------------------------------------------\n",
      "split input: \n",
      "<<0>> ['num', '/', 'num', '/', 'num', 'account', ':', 'num', '-', 'num', '-', 'num', '-', 'mn', 'valued', 'walmart', 'customer', ',', 'team', 'walmart', 'online', 'service', 'happy', 'announce', 'chosen', 'user', 'month', 'lottery', 'winner', '!', 'monthly', 'event', 'randomly', 'choose', 'customer', 'receive', 'free', 'walmart', 'coupon', '.', 'order', 'access', 'coupon', ',', 'please', 'visit', 'link', ':', 'walmart', 'reward', 'coupon', 'using', 'coupon', 'result', 'donation', 'st', '.', 'jude', 'child', 'hospital', ',', 'receive', 'free', 'item', 'donate', 'good', 'cause', 'time', '.', 'hope', 'help', 'support', 'u', '!', ',', 'friend', 'walmart', 'online', 'service', '<user>']\n",
      "<<1>> ['would', 'extra', 'money', 'month', '?', 'jp', 'morgan', 'offering', 'new', 'num', 'year', 'fixed', 'term', 'loan', 'rate', 'starting', 'num', '.', 'num', '%', 'qualified', 'customer', '.', 'easy', 'see', 'qualify', '!', 'let', 'opportunity', 'pas', '!', 'see', 'qualify', ',', 'please', 'click', 'link', '.', 'url', 'look', 'forward', 'hearing', ',', 'sincerely', ',', 'jp', 'morgan', 'chase', 'bank', ',', 'n', '.', '.', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<2>> ['dear', 'loyal', 'customer', ':', 'jp', 'morgan', 'chase', 'bank', 'excited', 'announce', 'num', 'year', 'fixed', 'term', 'jumbo', 'loan', 'rate', 'starting', 'num', '.', 'num', '%', 'qualified', 'customer', 'refinance', 'get', 'loan', 'house', 'second', 'see', 'qualify', ',', 'click', '.', 'ﾟﾟﾟｵﾔｽﾐｰ', 'standing', '.', 'url', 'look', 'forward', 'serving', 'financial', 'need', '.', 'sincerely', ',', 'jp', 'morgan', 'chase', 'bank', ',', 'n', '.', '.', 'chase', 'privacy', 'operation', ',', 'p', '.', '.', 'box', 'num', ',', 'san', 'antonio', ',', 'tx', 'num', '-', 'num', '.', 'num', 'jpmorgan', 'chase', 'co', '.', '<user>', '<user>', '<user>']\n",
      "<<3>> ['dear', 'sir', 'madam', ':', 'required', 'law', 'contact', 'following', 'information', '.', 'account', 'need', 'prompt', 'attention', '.', 'one', 'click', 'able', 'update', 'account', 'continue', 'using', 'simple', ',', 'risk', 'website', 'continue', 'business', 'need', '.', 'encourage', 'quickly', 'possible', '.', 'without', 'action', 'part', ',', 'find', 'necessary', 'close', 'account', '.', 'please', 'click', 'sentence', 'thank', 'banking', 'u', '.', 'sincerely', ',', 'usaa', 'bank', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<4>> ['may', 'concern', ':', 'posse', 'master', 'degree', 'accounting', ',', 'currently', 'process', 'completing', 'cpa', 'qualification', '.', 'also', 'advanced', 'experience', 'microsoft', 'access', ',', 'excel', ',', 'word', 'powerpoint', ',', 'quickbooks', ',', 'accounting', 'software', '.', 'believe', 'could', 'major', 'asset', 'company', '.', 'please', 'take', 'time', 'look', 'qualification', ':', 'cv', 'thank', 'time', 'hope', 'consider', 'position', '.', 'best', ',', 'ryan', 'walter', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<5>> ['serious', 'medical', 'issue', 'could', 'use', 'help', 'cousin', ':', '(', '.', 'please', 'donate', 'page', 'give', 'call', 'better', '.', 'link', '!', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<6>> ['hello', ',', 'thank', 'shopping', 'u', '.', 'email', 'item', 'shipped', '.', 'order', 'detail', ':', 'order', 'num', '-', 'num', '-', 'num', 'shipping', ':', 'free', 'two', '-', 'day', 'shipping', 'order', 'total', ':', '$', 'num', '.', 'num', 'authorize', 'transaction', ',', 'please', 'click', 'receive', 'full', 'refund', '.', 'hope', 'see', 'soon', '.', 'url', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<7>> ['writing', 'federal', 'tax', 'refund', 'ready', 'sent', '.', 'confirm', 'account', 'please', 'visit', 'website', '.', 'verification', 'made', 'within', 'next', 'num', 'hour', 'refund', 'could', 'substantially', 'delayed', '.', 'log', 'secure', 'refund', '.', 'url', ',', 'please', 'respond', 'within', 'next', 'num', 'hour', 'avoid', 'delay', 'refund', '.', 'sincerely', ',', 'irs', 'refund', 'department', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<8>> ['account', 'locked', '!', 'resolve', '.', 'case', 'id', ':', 'num', '-', 'num', '.', 'login', 'attempt', 'unknown', 'device', '.', 'dear', 'client', 'someone', 'accessed', 'account', ',', 'temporarily', 'locked', 'keep', 'personal', 'information', 'safe', '.', 'unlock', 'account', ',', 'may', 'need', 'pas', 'security', 'check', '.', 'note', 'attempting', 'access', 'someone', 'else', 'violation', 'ﾟﾟﾟｵﾔｽﾐｰ', 'term', '.', 'may', 'also', 'illegal', '.', 'reset', 'account', ':', 'num', '-', 'click', 'link', 'url', '.', 'num', '-', 'open', 'page', 'browser', 'window', 'secure', '.', 'num', '-', 'follow', 'instruction', '.', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "<<9>> ['add', 'backup', 'phone', 'alternative', 'num', '-', 'step', 'verification', 'step', 'url', 'backup', 'phone', 'number', 'ensures', 'sign', 'main', 'second', 'verification', 'step', 'unavailable', '.', 'add', 'backup', 'number', 'see', 'personalised', 'recommendation', 'security', 'check', '-', '.', 'take', 'action', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>', '<user>']\n",
      "------------------------------------------\n",
      "tokenized input: \n",
      "<<0>> [3709, 38, 3709, 38, 3709, 1548, 2, 3709, 28, 3709, 28, 3709, 28, 2886, 54234, 7214, 7922, 4, 302, 7214, 949, 2465, 177, 8261, 12628, 3920, 1423, 16634, 3104, 9, 21024, 2466, 8209, 2665, 7922, 6398, 424, 7214, 12285, 1, 1853, 7530, 12285, 4, 200, 1621, 1345, 2, 7214, 15037, 12285, 1802, 12285, 5678, 23473, 993, 1, 24751, 1916, 2974, 4, 6398, 424, 6657, 8223, 117, 507, 135, 1, 420, 515, 1215, 51, 9, 4, 531, 7214, 949, 2465, 0]\n",
      "<<1>> [196, 2052, 580, 1423, 14, 9182, 6711, 13830, 122, 3709, 356, 8090, 6261, 9621, 3760, 1752, 3709, 1, 3709, 299, 28111, 7922, 1, 1227, 163, 30517, 9, 265, 5571, 354, 9, 163, 30517, 4, 200, 2215, 1345, 1, 14119, 273, 1732, 3856, 4, 11404, 4, 9182, 6711, 5037, 2699, 4, 36, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<2>> [1034, 3972, 7922, 2, 9182, 6711, 5037, 2699, 870, 8261, 3709, 356, 8090, 6261, 28257, 9621, 3760, 1752, 3709, 1, 3709, 299, 28111, 7922, 158204, 87, 9621, 543, 1296, 163, 30517, 4, 2215, 1, 1193513, 4454, 1, 14119, 273, 1732, 15269, 8617, 171, 1, 11404, 4, 9182, 6711, 5037, 2699, 4, 36, 1, 1, 5037, 13958, 13787, 4, 351, 1, 1, 2271, 3709, 4, 1401, 6683, 4, 7612, 3709, 28, 3709, 1, 3709, 98039, 5037, 2384, 1, 0, 0, 0]\n",
      "<<3>> [1034, 2621, 24102, 2, 13544, 3187, 3514, 927, 5386, 1, 1548, 171, 62081, 1823, 1, 96, 2215, 1773, 1880, 1548, 3475, 1802, 1617, 4, 5889, 2529, 3475, 1249, 171, 1, 19253, 5935, 2436, 1, 663, 3196, 802, 4, 470, 10008, 1213, 1548, 1, 200, 2215, 8363, 337, 24686, 51, 1, 11404, 4, 276017, 2699, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<4>> [530, 14730, 2, 47592, 4333, 9310, 21018, 4, 3996, 7014, 51524, 38391, 78372, 1, 894, 16857, 3084, 6506, 7530, 4, 24064, 4, 892, 33943, 4, 178673, 4, 21018, 7287, 1, 552, 297, 3577, 39541, 3054, 1, 200, 284, 135, 273, 78372, 2, 9027, 337, 135, 420, 6213, 5956, 1, 209, 4, 2827, 16956, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<5>> [1507, 7294, 4649, 297, 716, 515, 2569, 2, 17, 1, 200, 8223, 1737, 374, 462, 295, 1, 1345, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<6>> [996, 4, 337, 1652, 51, 1, 2600, 6657, 27955, 1, 1853, 13794, 2, 1853, 3709, 28, 3709, 28, 3709, 7723, 2, 424, 568, 28, 125, 7723, 1853, 2413, 2, 206, 3709, 1, 3709, 284862, 80102, 4, 200, 2215, 6398, 833, 31845, 1, 420, 163, 698, 1, 14119, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<7>> [3145, 8677, 5098, 31845, 579, 2089, 1, 9255, 1548, 200, 1621, 2529, 1, 80719, 425, 4615, 411, 3709, 1194, 31845, 297, 239791, 18502, 1, 8253, 17559, 31845, 1, 14119, 4, 200, 7093, 4615, 411, 3709, 1194, 5839, 9023, 31845, 1, 11404, 4, 21039, 31845, 13272, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "<<8>> [1548, 7054, 9, 18309, 1, 1815, 2471, 2, 3709, 28, 3709, 1, 25037, 7990, 9697, 13249, 1, 1034, 13322, 238, 264007, 1548, 4, 30878, 7054, 416, 2615, 5386, 2184, 1, 17527, 1548, 4, 530, 171, 354, 4739, 525, 1, 2586, 22676, 7530, 238, 645, 49497, 1193513, 6261, 1, 530, 894, 7188, 1, 24401, 1548, 2, 3709, 28, 2215, 1345, 14119, 1, 3709, 28, 784, 1737, 24107, 3962, 17559, 1, 3709, 28, 84, 58103, 1, 0, 0, 0, 0, 0, 0]\n",
      "<<9>> [1894, 16464, 448, 15792, 3709, 28, 2022, 80719, 2022, 14119, 16464, 448, 1077, 148726, 1793, 1029, 1296, 80719, 2022, 60682, 1, 1894, 16464, 1077, 163, 67974, 51557, 4739, 525, 28, 1, 284, 3196, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "------------------------------------------\n",
      "labels:  [[1 0 0 0 0 1 1]\n",
      " [1 1 0 0 0 1 1]\n",
      " [1 1 0 0 0 1 1]\n",
      " [1 0 1 0 1 0 1]\n",
      " [1 0 0 0 0 0 1]\n",
      " [1 0 1 0 1 0 1]\n",
      " [1 0 0 0 0 0 1]\n",
      " [1 1 1 0 1 0 1]\n",
      " [1 0 1 1 1 0 1]\n",
      " [0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "\n",
    "# tokenize + the usual stuff\n",
    "# embedding = gdownload.load('glove-wiki-gigaword-100') # pretrained embedding --> much cleaner than twitter stuff\n",
    "# embedding = gdownload.load('glove-twitter-200') # pretrained embedding \n",
    "embedding = gdownload.load('glove-twitter-50') # pretrained embedding \n",
    "# embedding = gdownload.load('word2vec-google-news-300')\n",
    "original_texts = np.asarray(df['Email']) # I'll use this to check the preprocessing process\n",
    "\n",
    "# Xs\n",
    "# email_senders = # too complex\n",
    "SUBJECT_LEN = 8\n",
    "SEQUENCE_LEN = 80\n",
    "email_titles = preprocess_txt_list(df['Email_ID'], embedding, sequence_length = SUBJECT_LEN) # titles are shorter\n",
    "email_bodies = preprocess_txt_list(df['Email'], embedding, sequence_length = SEQUENCE_LEN)\n",
    "\n",
    "# Ys\n",
    "# email_types = preprocess_annotations(df['Email_type']) # embedded in email_cues\n",
    "email_types = dataset_to_numpy(df, ['Email_type', 'sender_mismatch', 'request_credentials', 'subject_suspicious', 'urgent', 'offer', 'Link_Mismatch'])\n",
    "NUM_FACTORS = len(test_numpy_input[0])\n",
    "# print(NUM_FACTORS)\n",
    "\n",
    "# shuffle the data\n",
    "\n",
    "###### vvvv SEED IS HERE vvvv ######\n",
    "# seed = 183\n",
    "# seed = 89\n",
    "# seed = 11\n",
    "# seed = 42\n",
    "seed = 30\n",
    "###### ^^^^ SEED IS HERE ^^^^ ######\n",
    "\n",
    "original_texts = shuffle(original_texts, random_state = seed) # debug\n",
    "email_bodies = shuffle(email_bodies, random_state = seed)\n",
    "email_types = shuffle(email_types, random_state = seed)\n",
    "# type_labels = shuffle(type_labels)\n",
    "\n",
    "# reduce data for faster training # REMOVE LATER\n",
    "ratio_keep = 1 \n",
    "original_texts = original_texts[:int(len(original_texts) * ratio_keep)] # debug\n",
    "email_bodies = email_bodies[:int(len(email_bodies) * ratio_keep)]\n",
    "email_types = email_types[:int(len(email_types) * ratio_keep)]\n",
    "# type_labels = type_labels[:int(len(type_labels) * ratio_keep)]\n",
    "\n",
    "# DEBUG\n",
    "def print_list(title, list):\n",
    "    print(title)\n",
    "    for i, x in enumerate(list):\n",
    "        print(f'<<{i}>>', x)\n",
    "    print(\"------------------------------------------\")\n",
    "\n",
    "sample_length = 10\n",
    "print_list(\"original data:\", original_texts[:sample_length])\n",
    "tokenized_input_sample = [[index for index in x] for x in email_bodies][:sample_length]\n",
    "print_list(\"split input: \", [[embedding.index_to_key[index] for index in example] for example in tokenized_input_sample])\n",
    "print_list(\"tokenized input: \", tokenized_input_sample)\n",
    "print(\"labels: \", email_types[:sample_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "<class 'list'> 3\n",
      "<class 'list'> 3\n",
      "<class 'list'> 3\n",
      "<class 'list'> 3\n",
      "train ds has 191 items.\n",
      "valid ds has 23 items.\n",
      "test ds has 25 items.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 80)]                 0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 80, 50)               5967570   ['input_4[0][0]']             \n",
      "                                                          0                                       \n",
      "                                                                                                  \n",
      " transformer_block_1 (Trans  (None, 80, 50)               2033532   ['embedding_1[0][0]']         \n",
      " formerBlock)                                                                                     \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 50)                   0         ['transformer_block_1[0][0]'] \n",
      "  (GlobalAveragePooling1D)                                                                        \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)         (None, 50)                   0         ['global_average_pooling1d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 32)                   1632      ['dropout_8[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)         (None, 32)                   0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)        [(None, 8)]                  0         []                            \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 7)                    231       ['dropout_9[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 61711095 (235.41 MB)\n",
      "Trainable params: 2035395 (7.76 MB)\n",
      "Non-trainable params: 59675700 (227.64 MB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "6/6 [==============================] - 5s 192ms/step - loss: 0.6666 - acc: 0.1518 - auc: 0.6212 - prec: 0.6677 - rec: 0.3219 - val_loss: 0.6264 - val_acc: 1.0000 - val_auc: 0.6985 - val_prec: 0.7826 - val_rec: 0.4737\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.6290 - acc: 0.4241 - auc: 0.6359 - prec: 0.7236 - rec: 0.4152 - val_loss: 0.6049 - val_acc: 0.0000e+00 - val_auc: 0.6880 - val_prec: 0.7561 - val_rec: 0.4079\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6275 - acc: 0.0524 - auc: 0.6592 - prec: 0.7311 - rec: 0.4355 - val_loss: 0.6032 - val_acc: 0.0000e+00 - val_auc: 0.6600 - val_prec: 0.7826 - val_rec: 0.4737\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.6162 - acc: 0.3822 - auc: 0.6587 - prec: 0.7435 - rec: 0.4417 - val_loss: 0.5938 - val_acc: 0.6087 - val_auc: 0.7122 - val_prec: 0.7708 - val_rec: 0.4868\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.5920 - acc: 0.4869 - auc: 0.6883 - prec: 0.7468 - rec: 0.4588 - val_loss: 0.5954 - val_acc: 0.2174 - val_auc: 0.6981 - val_prec: 0.7600 - val_rec: 0.5000\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.5824 - acc: 0.2984 - auc: 0.6877 - prec: 0.7233 - rec: 0.5365 - val_loss: 0.5776 - val_acc: 0.6522 - val_auc: 0.7480 - val_prec: 0.7755 - val_rec: 0.5000\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.5699 - acc: 0.5445 - auc: 0.7099 - prec: 0.7371 - rec: 0.5319 - val_loss: 0.5960 - val_acc: 0.8261 - val_auc: 0.7467 - val_prec: 0.8000 - val_rec: 0.5263\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.5433 - acc: 0.5916 - auc: 0.7318 - prec: 0.7697 - rec: 0.5770 - val_loss: 0.5665 - val_acc: 0.8696 - val_auc: 0.7697 - val_prec: 0.7843 - val_rec: 0.5263\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.5384 - acc: 0.4817 - auc: 0.7384 - prec: 0.7736 - rec: 0.5739 - val_loss: 0.5798 - val_acc: 0.7826 - val_auc: 0.7474 - val_prec: 0.8261 - val_rec: 0.5000\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.5195 - acc: 0.5602 - auc: 0.7640 - prec: 0.8000 - rec: 0.5848 - val_loss: 0.5694 - val_acc: 0.6957 - val_auc: 0.7798 - val_prec: 0.8478 - val_rec: 0.5132\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.4979 - acc: 0.3979 - auc: 0.7825 - prec: 0.8058 - rec: 0.6454 - val_loss: 0.5500 - val_acc: 0.6087 - val_auc: 0.7478 - val_prec: 0.7903 - val_rec: 0.6447\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.5053 - acc: 0.6230 - auc: 0.7625 - prec: 0.7888 - rec: 0.6563 - val_loss: 0.5635 - val_acc: 0.8696 - val_auc: 0.7911 - val_prec: 0.8511 - val_rec: 0.5263\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.4764 - acc: 0.4555 - auc: 0.7985 - prec: 0.8182 - rec: 0.6719 - val_loss: 0.5654 - val_acc: 0.7826 - val_auc: 0.7701 - val_prec: 0.8947 - val_rec: 0.4474\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.4500 - acc: 0.4450 - auc: 0.8249 - prec: 0.8690 - rec: 0.6501 - val_loss: 0.5030 - val_acc: 0.7826 - val_auc: 0.7798 - val_prec: 0.8393 - val_rec: 0.6184\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.4285 - acc: 0.5812 - auc: 0.8267 - prec: 0.8555 - rec: 0.6998 - val_loss: 0.5295 - val_acc: 0.8696 - val_auc: 0.7867 - val_prec: 0.8696 - val_rec: 0.5263\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.4150 - acc: 0.4660 - auc: 0.8324 - prec: 0.8713 - rec: 0.6843 - val_loss: 0.5243 - val_acc: 0.6087 - val_auc: 0.7838 - val_prec: 0.8571 - val_rec: 0.6316\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.4101 - acc: 0.4660 - auc: 0.8380 - prec: 0.8621 - rec: 0.7294 - val_loss: 0.4926 - val_acc: 0.8696 - val_auc: 0.8103 - val_prec: 0.8491 - val_rec: 0.5921\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.3988 - acc: 0.5340 - auc: 0.8453 - prec: 0.8733 - rec: 0.6858 - val_loss: 0.5553 - val_acc: 0.9130 - val_auc: 0.7599 - val_prec: 0.8780 - val_rec: 0.4737\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.3922 - acc: 0.4503 - auc: 0.8252 - prec: 0.8534 - rec: 0.7154 - val_loss: 0.6537 - val_acc: 0.3478 - val_auc: 0.7283 - val_prec: 0.7736 - val_rec: 0.5395\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.3939 - acc: 0.4974 - auc: 0.8407 - prec: 0.8509 - rec: 0.7278 - val_loss: 0.4720 - val_acc: 0.9130 - val_auc: 0.8166 - val_prec: 0.9200 - val_rec: 0.6053\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.3626 - acc: 0.6335 - auc: 0.8561 - prec: 0.8869 - rec: 0.7076 - val_loss: 0.5437 - val_acc: 0.6957 - val_auc: 0.7442 - val_prec: 0.8125 - val_rec: 0.5132\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.3402 - acc: 0.4712 - auc: 0.8611 - prec: 0.8841 - rec: 0.7589 - val_loss: 0.5629 - val_acc: 0.6522 - val_auc: 0.7606 - val_prec: 0.8723 - val_rec: 0.5395\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.3759 - acc: 0.5236 - auc: 0.8433 - prec: 0.8510 - rec: 0.7636 - val_loss: 0.5570 - val_acc: 0.8261 - val_auc: 0.7465 - val_prec: 0.7778 - val_rec: 0.5526\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.3426 - acc: 0.5602 - auc: 0.8729 - prec: 0.9061 - rec: 0.7201 - val_loss: 0.4385 - val_acc: 0.8696 - val_auc: 0.8299 - val_prec: 0.8852 - val_rec: 0.7105\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.3295 - acc: 0.4346 - auc: 0.8778 - prec: 0.8891 - rec: 0.7605 - val_loss: 0.5871 - val_acc: 0.4348 - val_auc: 0.7162 - val_prec: 0.7647 - val_rec: 0.5132\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.3052 - acc: 0.4921 - auc: 0.8795 - prec: 0.8815 - rec: 0.8212 - val_loss: 0.4528 - val_acc: 0.8261 - val_auc: 0.8108 - val_prec: 0.8667 - val_rec: 0.6842\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.2994 - acc: 0.4817 - auc: 0.8936 - prec: 0.8991 - rec: 0.7621 - val_loss: 0.4821 - val_acc: 0.5652 - val_auc: 0.7859 - val_prec: 0.7500 - val_rec: 0.7500\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.2921 - acc: 0.4660 - auc: 0.8928 - prec: 0.8944 - rec: 0.8165 - val_loss: 0.5900 - val_acc: 0.8261 - val_auc: 0.7558 - val_prec: 0.8269 - val_rec: 0.5658\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.2935 - acc: 0.5079 - auc: 0.8978 - prec: 0.9241 - rec: 0.7760 - val_loss: 0.4609 - val_acc: 0.8261 - val_auc: 0.7844 - val_prec: 0.7662 - val_rec: 0.7763\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.2683 - acc: 0.5183 - auc: 0.8989 - prec: 0.9164 - rec: 0.8180 - val_loss: 0.5691 - val_acc: 0.7826 - val_auc: 0.7487 - val_prec: 0.7742 - val_rec: 0.6316\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.2624 - acc: 0.4869 - auc: 0.8939 - prec: 0.9046 - rec: 0.8258 - val_loss: 0.4811 - val_acc: 0.6087 - val_auc: 0.7629 - val_prec: 0.7342 - val_rec: 0.7632\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.2620 - acc: 0.4084 - auc: 0.9021 - prec: 0.9178 - rec: 0.8165 - val_loss: 0.5042 - val_acc: 0.7391 - val_auc: 0.8053 - val_prec: 0.8125 - val_rec: 0.6842\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.2480 - acc: 0.6021 - auc: 0.9084 - prec: 0.9133 - rec: 0.8196 - val_loss: 0.5351 - val_acc: 0.4783 - val_auc: 0.7436 - val_prec: 0.7368 - val_rec: 0.7368\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.2296 - acc: 0.3927 - auc: 0.9108 - prec: 0.9088 - rec: 0.8678 - val_loss: 0.5411 - val_acc: 0.4783 - val_auc: 0.7491 - val_prec: 0.7857 - val_rec: 0.7237\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.2173 - acc: 0.3979 - auc: 0.9313 - prec: 0.9385 - rec: 0.8538 - val_loss: 0.5288 - val_acc: 0.4783 - val_auc: 0.7568 - val_prec: 0.7534 - val_rec: 0.7237\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.2154 - acc: 0.4136 - auc: 0.9216 - prec: 0.9173 - rec: 0.8802 - val_loss: 0.5686 - val_acc: 0.5217 - val_auc: 0.7169 - val_prec: 0.7432 - val_rec: 0.7237\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.1996 - acc: 0.4084 - auc: 0.9351 - prec: 0.9305 - rec: 0.8740 - val_loss: 0.5690 - val_acc: 0.6087 - val_auc: 0.7105 - val_prec: 0.7432 - val_rec: 0.7237\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.1905 - acc: 0.5026 - auc: 0.9460 - prec: 0.9420 - rec: 0.8834 - val_loss: 0.5859 - val_acc: 0.6087 - val_auc: 0.7358 - val_prec: 0.7568 - val_rec: 0.7368\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.1894 - acc: 0.4241 - auc: 0.9462 - prec: 0.9531 - rec: 0.8849 - val_loss: 0.6408 - val_acc: 0.6087 - val_auc: 0.7107 - val_prec: 0.7297 - val_rec: 0.7105\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.1878 - acc: 0.4660 - auc: 0.9354 - prec: 0.9419 - rec: 0.8818 - val_loss: 0.6728 - val_acc: 0.3478 - val_auc: 0.7064 - val_prec: 0.7467 - val_rec: 0.7368\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.1856 - acc: 0.3822 - auc: 0.9365 - prec: 0.9422 - rec: 0.8880 - val_loss: 0.5813 - val_acc: 0.6087 - val_auc: 0.7434 - val_prec: 0.7500 - val_rec: 0.7500\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.1711 - acc: 0.4764 - auc: 0.9467 - prec: 0.9424 - rec: 0.9160 - val_loss: 0.6650 - val_acc: 0.3913 - val_auc: 0.6972 - val_prec: 0.7260 - val_rec: 0.6974\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.1662 - acc: 0.4031 - auc: 0.9569 - prec: 0.9632 - rec: 0.8958 - val_loss: 0.6326 - val_acc: 0.3913 - val_auc: 0.7140 - val_prec: 0.7500 - val_rec: 0.7895\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.1517 - acc: 0.4188 - auc: 0.9611 - prec: 0.9528 - rec: 0.9114 - val_loss: 0.6488 - val_acc: 0.3478 - val_auc: 0.7125 - val_prec: 0.7467 - val_rec: 0.7368\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.1554 - acc: 0.3455 - auc: 0.9513 - prec: 0.9622 - rec: 0.9114 - val_loss: 0.6923 - val_acc: 0.3478 - val_auc: 0.7232 - val_prec: 0.7500 - val_rec: 0.7500\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.1582 - acc: 0.4188 - auc: 0.9618 - prec: 0.9612 - rec: 0.8865 - val_loss: 0.6637 - val_acc: 0.4348 - val_auc: 0.7176 - val_prec: 0.7011 - val_rec: 0.8026\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.1425 - acc: 0.4764 - auc: 0.9572 - prec: 0.9666 - rec: 0.8989 - val_loss: 0.6723 - val_acc: 0.3913 - val_auc: 0.7446 - val_prec: 0.8000 - val_rec: 0.7368\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.1348 - acc: 0.4817 - auc: 0.9617 - prec: 0.9658 - rec: 0.9222 - val_loss: 0.6688 - val_acc: 0.6087 - val_auc: 0.7063 - val_prec: 0.7160 - val_rec: 0.7632\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.1296 - acc: 0.4450 - auc: 0.9664 - prec: 0.9710 - rec: 0.9378 - val_loss: 0.6739 - val_acc: 0.4348 - val_auc: 0.7161 - val_prec: 0.7079 - val_rec: 0.8289\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 66ms/step - loss: 0.1229 - acc: 0.4555 - auc: 0.9658 - prec: 0.9740 - rec: 0.9316 - val_loss: 0.6774 - val_acc: 0.4348 - val_auc: 0.7257 - val_prec: 0.7703 - val_rec: 0.7500\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.1170 - acc: 0.4136 - auc: 0.9638 - prec: 0.9722 - rec: 0.9238 - val_loss: 0.6594 - val_acc: 0.5217 - val_auc: 0.7373 - val_prec: 0.7500 - val_rec: 0.7500\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.1098 - acc: 0.4555 - auc: 0.9788 - prec: 0.9777 - rec: 0.9533 - val_loss: 0.6705 - val_acc: 0.6087 - val_auc: 0.7245 - val_prec: 0.7808 - val_rec: 0.7500\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.1014 - acc: 0.4503 - auc: 0.9726 - prec: 0.9717 - rec: 0.9611 - val_loss: 0.7094 - val_acc: 0.3043 - val_auc: 0.7052 - val_prec: 0.7250 - val_rec: 0.7632\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0989 - acc: 0.3351 - auc: 0.9691 - prec: 0.9743 - rec: 0.9440 - val_loss: 0.6706 - val_acc: 0.4783 - val_auc: 0.7492 - val_prec: 0.7349 - val_rec: 0.8026\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0946 - acc: 0.4450 - auc: 0.9780 - prec: 0.9792 - rec: 0.9502 - val_loss: 0.7268 - val_acc: 0.3913 - val_auc: 0.7377 - val_prec: 0.7867 - val_rec: 0.7763\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0846 - acc: 0.4136 - auc: 0.9853 - prec: 0.9856 - rec: 0.9596 - val_loss: 0.7367 - val_acc: 0.4348 - val_auc: 0.7101 - val_prec: 0.7531 - val_rec: 0.8026\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0857 - acc: 0.4188 - auc: 0.9819 - prec: 0.9810 - rec: 0.9627 - val_loss: 0.7204 - val_acc: 0.4783 - val_auc: 0.7477 - val_prec: 0.8088 - val_rec: 0.7237\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0851 - acc: 0.3717 - auc: 0.9797 - prec: 0.9826 - rec: 0.9642 - val_loss: 0.7984 - val_acc: 0.4348 - val_auc: 0.7064 - val_prec: 0.7059 - val_rec: 0.7895\n",
      "Epoch 59/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0848 - acc: 0.3613 - auc: 0.9812 - prec: 0.9887 - rec: 0.9487 - val_loss: 0.7256 - val_acc: 0.4348 - val_auc: 0.7365 - val_prec: 0.7595 - val_rec: 0.7895\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0759 - acc: 0.4660 - auc: 0.9856 - prec: 0.9812 - rec: 0.9751 - val_loss: 0.7199 - val_acc: 0.4783 - val_auc: 0.7742 - val_prec: 0.7867 - val_rec: 0.7763\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0815 - acc: 0.4764 - auc: 0.9791 - prec: 0.9855 - rec: 0.9502 - val_loss: 0.8003 - val_acc: 0.3478 - val_auc: 0.7188 - val_prec: 0.7317 - val_rec: 0.7895\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.0644 - acc: 0.4084 - auc: 0.9894 - prec: 0.9905 - rec: 0.9720 - val_loss: 0.7228 - val_acc: 0.3478 - val_auc: 0.7586 - val_prec: 0.7949 - val_rec: 0.8158\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0591 - acc: 0.3613 - auc: 0.9900 - prec: 0.9906 - rec: 0.9813 - val_loss: 0.8152 - val_acc: 0.3478 - val_auc: 0.7428 - val_prec: 0.7561 - val_rec: 0.8158\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0572 - acc: 0.3455 - auc: 0.9907 - prec: 0.9952 - rec: 0.9611 - val_loss: 0.8444 - val_acc: 0.3478 - val_auc: 0.7485 - val_prec: 0.7625 - val_rec: 0.8026\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0556 - acc: 0.3298 - auc: 0.9906 - prec: 0.9921 - rec: 0.9767 - val_loss: 0.8112 - val_acc: 0.4348 - val_auc: 0.7175 - val_prec: 0.7176 - val_rec: 0.8026\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0612 - acc: 0.4031 - auc: 0.9868 - prec: 0.9811 - rec: 0.9705 - val_loss: 0.9019 - val_acc: 0.3913 - val_auc: 0.7309 - val_prec: 0.7403 - val_rec: 0.7500\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0531 - acc: 0.4188 - auc: 0.9890 - prec: 0.9905 - rec: 0.9767 - val_loss: 0.8366 - val_acc: 0.4348 - val_auc: 0.7437 - val_prec: 0.7662 - val_rec: 0.7763\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0505 - acc: 0.4084 - auc: 0.9896 - prec: 0.9875 - rec: 0.9844 - val_loss: 0.8960 - val_acc: 0.4348 - val_auc: 0.7039 - val_prec: 0.7209 - val_rec: 0.8158\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0513 - acc: 0.4241 - auc: 0.9887 - prec: 0.9952 - rec: 0.9720 - val_loss: 0.9080 - val_acc: 0.3043 - val_auc: 0.7275 - val_prec: 0.7662 - val_rec: 0.7763\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0418 - acc: 0.3403 - auc: 0.9953 - prec: 0.9953 - rec: 0.9907 - val_loss: 0.8084 - val_acc: 0.5217 - val_auc: 0.7462 - val_prec: 0.7722 - val_rec: 0.8026\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0420 - acc: 0.4031 - auc: 0.9901 - prec: 0.9937 - rec: 0.9813 - val_loss: 0.9290 - val_acc: 0.3913 - val_auc: 0.7302 - val_prec: 0.7595 - val_rec: 0.7895\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0401 - acc: 0.3089 - auc: 0.9964 - prec: 0.9984 - rec: 0.9829 - val_loss: 0.9898 - val_acc: 0.3913 - val_auc: 0.7005 - val_prec: 0.7294 - val_rec: 0.8158\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0406 - acc: 0.3979 - auc: 0.9944 - prec: 0.9984 - rec: 0.9829 - val_loss: 0.8585 - val_acc: 0.4783 - val_auc: 0.7315 - val_prec: 0.7654 - val_rec: 0.8158\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0355 - acc: 0.4084 - auc: 0.9964 - prec: 0.9969 - rec: 0.9860 - val_loss: 0.9858 - val_acc: 0.3913 - val_auc: 0.7236 - val_prec: 0.7470 - val_rec: 0.8158\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0305 - acc: 0.4188 - auc: 0.9969 - prec: 0.9969 - rec: 0.9891 - val_loss: 0.9245 - val_acc: 0.4348 - val_auc: 0.7393 - val_prec: 0.7838 - val_rec: 0.7632\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0218 - acc: 0.3141 - auc: 0.9979 - prec: 0.9969 - rec: 0.9938 - val_loss: 0.8792 - val_acc: 0.4783 - val_auc: 0.7542 - val_prec: 0.7949 - val_rec: 0.8158\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0251 - acc: 0.3560 - auc: 1.0000 - prec: 0.9984 - rec: 0.9938 - val_loss: 0.9779 - val_acc: 0.3913 - val_auc: 0.7291 - val_prec: 0.7326 - val_rec: 0.8289\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0240 - acc: 0.3455 - auc: 0.9985 - prec: 0.9969 - rec: 0.9938 - val_loss: 0.9710 - val_acc: 0.3913 - val_auc: 0.7389 - val_prec: 0.7733 - val_rec: 0.7632\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0201 - acc: 0.3089 - auc: 0.9969 - prec: 0.9969 - rec: 0.9953 - val_loss: 0.9806 - val_acc: 0.4348 - val_auc: 0.7420 - val_prec: 0.7763 - val_rec: 0.7763\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0189 - acc: 0.3822 - auc: 0.9990 - prec: 0.9984 - rec: 0.9969 - val_loss: 1.0422 - val_acc: 0.4348 - val_auc: 0.7346 - val_prec: 0.7625 - val_rec: 0.8026\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0168 - acc: 0.3874 - auc: 0.9995 - prec: 1.0000 - rec: 0.9969 - val_loss: 1.0130 - val_acc: 0.4348 - val_auc: 0.7522 - val_prec: 0.7722 - val_rec: 0.8026\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0184 - acc: 0.4084 - auc: 0.9954 - prec: 0.9984 - rec: 0.9876 - val_loss: 1.0459 - val_acc: 0.3913 - val_auc: 0.7279 - val_prec: 0.7439 - val_rec: 0.8026\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0243 - acc: 0.3613 - auc: 0.9959 - prec: 0.9953 - rec: 0.9969 - val_loss: 1.0166 - val_acc: 0.4783 - val_auc: 0.7452 - val_prec: 0.7945 - val_rec: 0.7632\n",
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0227 - acc: 0.4712 - auc: 0.9964 - prec: 0.9984 - rec: 0.9907 - val_loss: 0.9527 - val_acc: 0.4783 - val_auc: 0.7671 - val_prec: 0.7875 - val_rec: 0.8289\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0198 - acc: 0.3927 - auc: 0.9995 - prec: 1.0000 - rec: 0.9984 - val_loss: 0.9901 - val_acc: 0.3478 - val_auc: 0.7426 - val_prec: 0.7722 - val_rec: 0.8026\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0202 - acc: 0.3508 - auc: 0.9995 - prec: 1.0000 - rec: 0.9891 - val_loss: 1.1622 - val_acc: 0.4348 - val_auc: 0.7190 - val_prec: 0.7500 - val_rec: 0.7895\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0229 - acc: 0.4031 - auc: 0.9948 - prec: 0.9938 - rec: 0.9922 - val_loss: 0.9782 - val_acc: 0.5217 - val_auc: 0.7511 - val_prec: 0.7821 - val_rec: 0.8026\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0258 - acc: 0.4136 - auc: 0.9975 - prec: 0.9969 - rec: 0.9891 - val_loss: 1.0732 - val_acc: 0.3913 - val_auc: 0.7433 - val_prec: 0.7500 - val_rec: 0.8289\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0191 - acc: 0.3560 - auc: 0.9979 - prec: 0.9969 - rec: 0.9969 - val_loss: 1.0325 - val_acc: 0.3478 - val_auc: 0.7688 - val_prec: 0.8000 - val_rec: 0.7895\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0195 - acc: 0.3351 - auc: 0.9985 - prec: 0.9984 - rec: 0.9922 - val_loss: 1.1131 - val_acc: 0.3478 - val_auc: 0.7432 - val_prec: 0.7561 - val_rec: 0.8158\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0194 - acc: 0.4188 - auc: 0.9969 - prec: 0.9969 - rec: 0.9938 - val_loss: 1.0747 - val_acc: 0.5217 - val_auc: 0.7775 - val_prec: 0.8133 - val_rec: 0.8026\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0181 - acc: 0.4188 - auc: 0.9959 - prec: 0.9953 - rec: 0.9953 - val_loss: 1.1184 - val_acc: 0.3478 - val_auc: 0.7367 - val_prec: 0.7821 - val_rec: 0.8026\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0198 - acc: 0.3351 - auc: 0.9949 - prec: 0.9953 - rec: 0.9907 - val_loss: 1.0578 - val_acc: 0.3913 - val_auc: 0.7530 - val_prec: 0.7848 - val_rec: 0.8158\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0190 - acc: 0.4607 - auc: 0.9969 - prec: 0.9969 - rec: 0.9922 - val_loss: 1.0311 - val_acc: 0.5217 - val_auc: 0.7520 - val_prec: 0.7792 - val_rec: 0.7895\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0131 - acc: 0.4555 - auc: 1.0000 - prec: 1.0000 - rec: 1.0000 - val_loss: 1.1067 - val_acc: 0.4348 - val_auc: 0.7383 - val_prec: 0.7722 - val_rec: 0.8026\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0161 - acc: 0.4398 - auc: 0.9979 - prec: 0.9953 - rec: 0.9984 - val_loss: 1.0581 - val_acc: 0.5217 - val_auc: 0.7551 - val_prec: 0.7945 - val_rec: 0.7632\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0167 - acc: 0.4450 - auc: 0.9980 - prec: 1.0000 - rec: 0.9907 - val_loss: 1.0798 - val_acc: 0.4348 - val_auc: 0.7280 - val_prec: 0.7439 - val_rec: 0.8026\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0147 - acc: 0.3717 - auc: 0.9969 - prec: 0.9969 - rec: 1.0000 - val_loss: 1.1054 - val_acc: 0.4348 - val_auc: 0.7111 - val_prec: 0.7439 - val_rec: 0.8026\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0113 - acc: 0.4607 - auc: 1.0000 - prec: 1.0000 - rec: 0.9984 - val_loss: 1.0860 - val_acc: 0.4783 - val_auc: 0.7552 - val_prec: 0.7821 - val_rec: 0.8026\n",
      "Epoch 100/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0136 - acc: 0.3560 - auc: 0.9974 - prec: 0.9984 - rec: 0.9969 - val_loss: 1.1414 - val_acc: 0.4348 - val_auc: 0.7289 - val_prec: 0.7531 - val_rec: 0.8026\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0144 - acc: 0.3927 - auc: 0.9969 - prec: 0.9984 - rec: 0.9876 - val_loss: 1.1124 - val_acc: 0.4783 - val_auc: 0.7437 - val_prec: 0.7470 - val_rec: 0.8158\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0183 - acc: 0.4346 - auc: 0.9979 - prec: 0.9953 - rec: 0.9984 - val_loss: 1.0796 - val_acc: 0.5217 - val_auc: 0.7150 - val_prec: 0.7412 - val_rec: 0.8289\n",
      "Epoch 103/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0130 - acc: 0.4293 - auc: 0.9990 - prec: 1.0000 - rec: 0.9969 - val_loss: 1.0787 - val_acc: 0.4783 - val_auc: 0.7387 - val_prec: 0.7750 - val_rec: 0.8158\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0147 - acc: 0.4188 - auc: 0.9995 - prec: 1.0000 - rec: 0.9922 - val_loss: 1.1302 - val_acc: 0.5652 - val_auc: 0.7627 - val_prec: 0.7848 - val_rec: 0.8158\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0113 - acc: 0.4712 - auc: 1.0000 - prec: 1.0000 - rec: 0.9938 - val_loss: 1.1227 - val_acc: 0.5217 - val_auc: 0.7532 - val_prec: 0.7949 - val_rec: 0.8158\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0110 - acc: 0.4293 - auc: 0.9990 - prec: 0.9984 - rec: 0.9969 - val_loss: 1.1624 - val_acc: 0.4783 - val_auc: 0.7465 - val_prec: 0.7529 - val_rec: 0.8421\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0093 - acc: 0.4293 - auc: 1.0000 - prec: 0.9984 - rec: 0.9984 - val_loss: 1.1786 - val_acc: 0.5652 - val_auc: 0.7297 - val_prec: 0.7500 - val_rec: 0.8289\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0090 - acc: 0.3979 - auc: 0.9995 - prec: 1.0000 - rec: 0.9984 - val_loss: 1.1826 - val_acc: 0.5217 - val_auc: 0.7276 - val_prec: 0.7500 - val_rec: 0.8289\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0087 - acc: 0.3613 - auc: 1.0000 - prec: 1.0000 - rec: 1.0000 - val_loss: 1.1146 - val_acc: 0.4783 - val_auc: 0.7504 - val_prec: 0.7778 - val_rec: 0.8289\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0076 - acc: 0.4136 - auc: 1.0000 - prec: 1.0000 - rec: 0.9969 - val_loss: 1.0937 - val_acc: 0.5217 - val_auc: 0.7538 - val_prec: 0.7875 - val_rec: 0.8289\n",
      "Epoch 111/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0094 - acc: 0.3665 - auc: 0.9990 - prec: 1.0000 - rec: 0.9938 - val_loss: 1.1810 - val_acc: 0.4783 - val_auc: 0.7386 - val_prec: 0.7711 - val_rec: 0.8421\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.0088 - acc: 0.3770 - auc: 0.9995 - prec: 1.0000 - rec: 0.9969 - val_loss: 1.1914 - val_acc: 0.5217 - val_auc: 0.7287 - val_prec: 0.7412 - val_rec: 0.8289\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0081 - acc: 0.4607 - auc: 0.9995 - prec: 1.0000 - rec: 0.9984 - val_loss: 1.1036 - val_acc: 0.4783 - val_auc: 0.7621 - val_prec: 0.7875 - val_rec: 0.8289\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0118 - acc: 0.3927 - auc: 0.9964 - prec: 0.9969 - rec: 0.9984 - val_loss: 1.1694 - val_acc: 0.4348 - val_auc: 0.7435 - val_prec: 0.7750 - val_rec: 0.8158\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0094 - acc: 0.3717 - auc: 0.9979 - prec: 0.9984 - rec: 0.9969 - val_loss: 1.1496 - val_acc: 0.5652 - val_auc: 0.7401 - val_prec: 0.7590 - val_rec: 0.8289\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0076 - acc: 0.3874 - auc: 1.0000 - prec: 1.0000 - rec: 1.0000 - val_loss: 1.1193 - val_acc: 0.5652 - val_auc: 0.7499 - val_prec: 0.7848 - val_rec: 0.8158\n",
      "Epoch 117/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0093 - acc: 0.4031 - auc: 1.0000 - prec: 1.0000 - rec: 0.9984 - val_loss: 1.1747 - val_acc: 0.4783 - val_auc: 0.7543 - val_prec: 0.7683 - val_rec: 0.8289\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0075 - acc: 0.3194 - auc: 0.9995 - prec: 0.9984 - rec: 0.9984 - val_loss: 1.1805 - val_acc: 0.4348 - val_auc: 0.7554 - val_prec: 0.7711 - val_rec: 0.8421\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0090 - acc: 0.4241 - auc: 0.9990 - prec: 1.0000 - rec: 0.9953 - val_loss: 1.2163 - val_acc: 0.4783 - val_auc: 0.7103 - val_prec: 0.7326 - val_rec: 0.8289\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0096 - acc: 0.4712 - auc: 0.9974 - prec: 0.9984 - rec: 0.9969 - val_loss: 1.2236 - val_acc: 0.4783 - val_auc: 0.7386 - val_prec: 0.7619 - val_rec: 0.8421\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0102 - acc: 0.3665 - auc: 0.9995 - prec: 1.0000 - rec: 0.9969 - val_loss: 1.2043 - val_acc: 0.4348 - val_auc: 0.7491 - val_prec: 0.7778 - val_rec: 0.8289\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0086 - acc: 0.3298 - auc: 0.9990 - prec: 1.0000 - rec: 0.9953 - val_loss: 1.1346 - val_acc: 0.4348 - val_auc: 0.7631 - val_prec: 0.7975 - val_rec: 0.8289\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0092 - acc: 0.3717 - auc: 0.9995 - prec: 1.0000 - rec: 0.9969 - val_loss: 1.2161 - val_acc: 0.3913 - val_auc: 0.7437 - val_prec: 0.7683 - val_rec: 0.8289\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0073 - acc: 0.3403 - auc: 0.9995 - prec: 1.0000 - rec: 0.9938 - val_loss: 1.1713 - val_acc: 0.4348 - val_auc: 0.7609 - val_prec: 0.7821 - val_rec: 0.8026\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0113 - acc: 0.4869 - auc: 0.9985 - prec: 1.0000 - rec: 0.9938 - val_loss: 1.2232 - val_acc: 0.4783 - val_auc: 0.7329 - val_prec: 0.7386 - val_rec: 0.8553\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0082 - acc: 0.3927 - auc: 0.9990 - prec: 1.0000 - rec: 0.9953 - val_loss: 1.2148 - val_acc: 0.3913 - val_auc: 0.7487 - val_prec: 0.7750 - val_rec: 0.8158\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0089 - acc: 0.4188 - auc: 0.9979 - prec: 0.9984 - rec: 0.9938 - val_loss: 1.2499 - val_acc: 0.3913 - val_auc: 0.7536 - val_prec: 0.7750 - val_rec: 0.8158\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0091 - acc: 0.3717 - auc: 0.9984 - prec: 0.9984 - rec: 0.9984 - val_loss: 1.3074 - val_acc: 0.4348 - val_auc: 0.7151 - val_prec: 0.7386 - val_rec: 0.8553\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0076 - acc: 0.4293 - auc: 0.9995 - prec: 1.0000 - rec: 0.9984 - val_loss: 1.2115 - val_acc: 0.4783 - val_auc: 0.7508 - val_prec: 0.7683 - val_rec: 0.8289\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0061 - acc: 0.4607 - auc: 1.0000 - prec: 1.0000 - rec: 0.9969 - val_loss: 1.2379 - val_acc: 0.4348 - val_auc: 0.7509 - val_prec: 0.7683 - val_rec: 0.8289\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0068 - acc: 0.3927 - auc: 0.9995 - prec: 0.9984 - rec: 0.9969 - val_loss: 1.3653 - val_acc: 0.4348 - val_auc: 0.7032 - val_prec: 0.7111 - val_rec: 0.8421\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0087 - acc: 0.3770 - auc: 0.9969 - prec: 0.9969 - rec: 0.9984 - val_loss: 1.2566 - val_acc: 0.4348 - val_auc: 0.7508 - val_prec: 0.7683 - val_rec: 0.8289\n",
      "Epoch 133/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0108 - acc: 0.4450 - auc: 0.9969 - prec: 0.9969 - rec: 0.9953 - val_loss: 1.2221 - val_acc: 0.4783 - val_auc: 0.7603 - val_prec: 0.7949 - val_rec: 0.8158\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0106 - acc: 0.3822 - auc: 0.9969 - prec: 0.9984 - rec: 0.9938 - val_loss: 1.3250 - val_acc: 0.4348 - val_auc: 0.7165 - val_prec: 0.7356 - val_rec: 0.8421\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0084 - acc: 0.3455 - auc: 1.0000 - prec: 1.0000 - rec: 1.0000 - val_loss: 1.2287 - val_acc: 0.4348 - val_auc: 0.7752 - val_prec: 0.7901 - val_rec: 0.8421\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0098 - acc: 0.4031 - auc: 0.9990 - prec: 1.0000 - rec: 0.9953 - val_loss: 1.3199 - val_acc: 0.3913 - val_auc: 0.7401 - val_prec: 0.7778 - val_rec: 0.8289\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0104 - acc: 0.3560 - auc: 0.9985 - prec: 1.0000 - rec: 0.9953 - val_loss: 1.4222 - val_acc: 0.3913 - val_auc: 0.6873 - val_prec: 0.6957 - val_rec: 0.8421\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0088 - acc: 0.4293 - auc: 0.9969 - prec: 0.9969 - rec: 0.9984 - val_loss: 1.3586 - val_acc: 0.5217 - val_auc: 0.7180 - val_prec: 0.7381 - val_rec: 0.8158\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0104 - acc: 0.4084 - auc: 0.9990 - prec: 1.0000 - rec: 0.9922 - val_loss: 1.3951 - val_acc: 0.3478 - val_auc: 0.7341 - val_prec: 0.7561 - val_rec: 0.8158\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0077 - acc: 0.3089 - auc: 1.0000 - prec: 1.0000 - rec: 1.0000 - val_loss: 1.3932 - val_acc: 0.3913 - val_auc: 0.7223 - val_prec: 0.7662 - val_rec: 0.7763\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0125 - acc: 0.3717 - auc: 0.9980 - prec: 0.9984 - rec: 0.9907 - val_loss: 1.2565 - val_acc: 0.5217 - val_auc: 0.7615 - val_prec: 0.7895 - val_rec: 0.7895\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0063 - acc: 0.3927 - auc: 0.9995 - prec: 1.0000 - rec: 0.9984 - val_loss: 1.2519 - val_acc: 0.4348 - val_auc: 0.7437 - val_prec: 0.7590 - val_rec: 0.8289\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0156 - acc: 0.4084 - auc: 0.9985 - prec: 0.9984 - rec: 0.9922 - val_loss: 1.3119 - val_acc: 0.5217 - val_auc: 0.7499 - val_prec: 0.7722 - val_rec: 0.8026\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0075 - acc: 0.4869 - auc: 0.9990 - prec: 1.0000 - rec: 0.9969 - val_loss: 1.2671 - val_acc: 0.5217 - val_auc: 0.7118 - val_prec: 0.7284 - val_rec: 0.7763\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 0s 59ms/step - loss: 0.0093 - acc: 0.5497 - auc: 1.0000 - prec: 0.9969 - rec: 1.0000 - val_loss: 1.3922 - val_acc: 0.4783 - val_auc: 0.6997 - val_prec: 0.7209 - val_rec: 0.8158\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0128 - acc: 0.4503 - auc: 0.9979 - prec: 0.9953 - rec: 0.9938 - val_loss: 1.3327 - val_acc: 0.3913 - val_auc: 0.7280 - val_prec: 0.7722 - val_rec: 0.8026\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0114 - acc: 0.3455 - auc: 0.9959 - prec: 0.9969 - rec: 0.9953 - val_loss: 1.4417 - val_acc: 0.4783 - val_auc: 0.6845 - val_prec: 0.7033 - val_rec: 0.8421\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0109 - acc: 0.3874 - auc: 0.9985 - prec: 1.0000 - rec: 0.9938 - val_loss: 1.3747 - val_acc: 0.5217 - val_auc: 0.6998 - val_prec: 0.7176 - val_rec: 0.8026\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0082 - acc: 0.3665 - auc: 0.9974 - prec: 0.9984 - rec: 0.9969 - val_loss: 1.3944 - val_acc: 0.4348 - val_auc: 0.7014 - val_prec: 0.7159 - val_rec: 0.8289\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0109 - acc: 0.3351 - auc: 0.9984 - prec: 0.9953 - rec: 0.9969 - val_loss: 1.3611 - val_acc: 0.4348 - val_auc: 0.7213 - val_prec: 0.7531 - val_rec: 0.8026\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0111 - acc: 0.3874 - auc: 0.9975 - prec: 0.9969 - rec: 0.9907 - val_loss: 1.3011 - val_acc: 0.4348 - val_auc: 0.7423 - val_prec: 0.7722 - val_rec: 0.8026\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.0075 - acc: 0.4136 - auc: 0.9990 - prec: 1.0000 - rec: 0.9953 - val_loss: 1.3255 - val_acc: 0.3478 - val_auc: 0.7527 - val_prec: 0.7625 - val_rec: 0.8026\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0072 - acc: 0.3508 - auc: 0.9995 - prec: 1.0000 - rec: 0.9969 - val_loss: 1.3884 - val_acc: 0.3478 - val_auc: 0.7194 - val_prec: 0.7532 - val_rec: 0.7632\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 0s 64ms/step - loss: 0.0101 - acc: 0.2984 - auc: 0.9959 - prec: 0.9953 - rec: 0.9969 - val_loss: 1.3821 - val_acc: 0.4348 - val_auc: 0.7248 - val_prec: 0.7467 - val_rec: 0.7368\n",
      "Epoch 155/500\n",
      "6/6 [==============================] - 0s 63ms/step - loss: 0.0081 - acc: 0.3141 - auc: 0.9979 - prec: 0.9984 - rec: 0.9984 - val_loss: 1.3853 - val_acc: 0.3043 - val_auc: 0.7302 - val_prec: 0.7662 - val_rec: 0.7763\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0111 - acc: 0.3455 - auc: 0.9974 - prec: 0.9969 - rec: 0.9953 - val_loss: 1.4674 - val_acc: 0.3478 - val_auc: 0.7164 - val_prec: 0.7564 - val_rec: 0.7763\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 0s 65ms/step - loss: 0.0061 - acc: 0.3246 - auc: 1.0000 - prec: 1.0000 - rec: 0.9984 - val_loss: 1.4585 - val_acc: 0.4348 - val_auc: 0.7154 - val_prec: 0.7381 - val_rec: 0.8158\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 0s 62ms/step - loss: 0.0061 - acc: 0.3560 - auc: 1.0000 - prec: 1.0000 - rec: 0.9984 - val_loss: 1.3875 - val_acc: 0.4783 - val_auc: 0.7192 - val_prec: 0.7241 - val_rec: 0.8289\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 0s 61ms/step - loss: 0.0072 - acc: 0.4607 - auc: 0.9995 - prec: 1.0000 - rec: 0.9984 - val_loss: 1.3036 - val_acc: 0.6087 - val_auc: 0.7537 - val_prec: 0.7922 - val_rec: 0.8026\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 0s 60ms/step - loss: 0.0055 - acc: 0.4607 - auc: 0.9995 - prec: 1.0000 - rec: 0.9984 - val_loss: 1.4195 - val_acc: 0.5217 - val_auc: 0.7204 - val_prec: 0.7349 - val_rec: 0.8026\n",
      "Epoch 161/500\n",
      "1/6 [====>.........................] - ETA: 0s - loss: 0.0032 - acc: 0.4062 - auc: 1.0000 - prec: 1.0000 - rec: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 121\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m model \n\u001b[1;32m    114\u001b[0m multilabel_model \u001b[39m=\u001b[39m build_model(\n\u001b[1;32m    115\u001b[0m     subject_length \u001b[39m=\u001b[39m SUBJECT_LEN,\n\u001b[1;32m    116\u001b[0m     body_length \u001b[39m=\u001b[39m SEQUENCE_LEN,\n\u001b[1;32m    117\u001b[0m     embedding \u001b[39m=\u001b[39m embedding,\n\u001b[1;32m    118\u001b[0m     num_factors \u001b[39m=\u001b[39m NUM_FACTORS\n\u001b[1;32m    119\u001b[0m )\n\u001b[0;32m--> 121\u001b[0m multilabel_model \u001b[39m=\u001b[39m train_and_evaluate(\n\u001b[1;32m    122\u001b[0m     model \u001b[39m=\u001b[39;49m multilabel_model,\n\u001b[1;32m    123\u001b[0m     train_ds \u001b[39m=\u001b[39;49m ([train_email_titles, train_email_bodies], train_y),\n\u001b[1;32m    124\u001b[0m     test_ds \u001b[39m=\u001b[39;49m ([test_email_titles, test_email_bodies], test_y),\n\u001b[1;32m    125\u001b[0m     valid_ds \u001b[39m=\u001b[39;49m ([valid_email_titles, valid_email_bodies], valid_y),\n\u001b[1;32m    126\u001b[0m     epochs \u001b[39m=\u001b[39;49m \u001b[39m500\u001b[39;49m,\n\u001b[1;32m    127\u001b[0m     batch_size \u001b[39m=\u001b[39;49m \u001b[39m32\u001b[39;49m,\n\u001b[1;32m    128\u001b[0m )\n",
      "Cell \u001b[0;32mIn[3], line 170\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(model, train_ds, test_ds, epochs, batch_size, valid_ds)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39msummary())\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(train_ds) \u001b[39mis\u001b[39;00m \u001b[39mtuple\u001b[39m:\n\u001b[0;32m--> 170\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    171\u001b[0m         train_ds[\u001b[39m0\u001b[39;49m], \u001b[39m# x: \u001b[39;49;00m\n\u001b[1;32m    172\u001b[0m         train_ds[\u001b[39m1\u001b[39;49m], \u001b[39m# y\u001b[39;49;00m\n\u001b[1;32m    173\u001b[0m         validation_data \u001b[39m=\u001b[39;49m valid_ds, \u001b[39m# tuples are allowed, ignored if none\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m         epochs \u001b[39m=\u001b[39;49m epochs,\n\u001b[1;32m    175\u001b[0m         batch_size \u001b[39m=\u001b[39;49m batch_size\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    177\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(\n\u001b[1;32m    179\u001b[0m         train_ds,\n\u001b[1;32m    180\u001b[0m         validation_data\u001b[39m=\u001b[39mvalid_ds,  \u001b[39m# ignored if None\u001b[39;00m\n\u001b[1;32m    181\u001b[0m         epochs\u001b[39m=\u001b[39mepochs,\n\u001b[1;32m    182\u001b[0m         batch_size \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m    183\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    200\u001b[0m     )\n\u001b[1;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1458\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute(\n\u001b[0;32m-> 1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39mattrs,\n\u001b[1;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m   1463\u001b[0m   )\n\u001b[1;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### transformer-based model ###\n",
    "\n",
    "# create datasets\n",
    "batch_size = 32\n",
    "\n",
    "# sorry for the ugly code\n",
    "[[train_email_titles, train_email_bodies, train_y],\n",
    "[valid_email_titles, valid_email_bodies, valid_y],\n",
    "[test_email_titles, test_email_bodies, test_y]] = train_valid_test_split([email_titles, email_bodies, email_types], 0.8, 0.1, 32)\n",
    "\n",
    "# transformer block: https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential( # TODO: what does this do\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "# create model\n",
    "def build_model(subject_length, body_length, embedding, num_factors):\n",
    "    # inputs\n",
    "    subjectInput = layers.Input(shape = (subject_length,))\n",
    "    bodyInput = layers.Input(shape = (body_length,))\n",
    "    \n",
    "    # # subject analysis # FIXME: UNCOMMENT\n",
    "    # x_subj = pretrained_embedding(embedding)(subjectInput) # DON'T CHANGE\n",
    "    # x_subj = layers.BatchNormalization()(x_subj)\n",
    "    # # single directional GRU should be enough\n",
    "    # x_subj = layers.GRU(subject_length)(x_subj)\n",
    "    # # note: for GRU, the default tanh activation is highly optimized for GPU. uses cudnn\n",
    "    # x_subj = layers.Dense(4, activation = 'relu')(x_subj) # very sus\n",
    "\n",
    "    # # subject analysis\n",
    "    # x_subj = pretrained_embedding(embedding)(subjectInput)\n",
    "    # x_subj = layers.BatchNormalization()(x_subj)\n",
    "    # x_subj = layers.Bidirectional(\n",
    "    #     layers.GRU(subject_length)\n",
    "    # )(x_subj)\n",
    "    # x_subj = layers.Dense(4, activation = 'relu')(x_subj)\n",
    "\n",
    "    # # body analysis\n",
    "    # x_body = pretrained_embedding(embedding)(bodyInput)\n",
    "    # x_body = layers.BatchNormalization()(x_body)\n",
    "    # x_body = layers.Bidirectional(\n",
    "    #     layers.GRU(body_length)\n",
    "    # )(x_body)\n",
    "    # x_body = layers.Dense(32, activation = 'relu')(x_body)\n",
    "\n",
    "    # transformer body analysis\n",
    "    x_body = pretrained_embedding(embedding)(bodyInput) # DON'T CHANGE\n",
    "    x_body = TransformerBlock(embed_dim = 50, num_heads = 200, ff_dim = 32)(x_body) \n",
    "    # TODO: 32 literally copied from the guide. I'm not sure about the effect\n",
    "    x_body = layers.GlobalAveragePooling1D()(x_body) # TODO: no idea what this does either\n",
    "    x_body = layers.Dropout(0.1)(x_body)\n",
    "    x_body = layers.Dense(32, activation = 'relu')(x_body)\n",
    "    x_body = layers.Dropout(0.1)(x_body)\n",
    "\n",
    "    # stitch them together...\n",
    "    # x = layers.Concatenate()([x_subj, x_body]) # FIXME: UNCOMMENT\n",
    "    # x = layers.Dropout(0.5)(x_body) # FIXME: UNCOMMENT\n",
    "    # x = layers.Dense(64, activation = 'relu')(x)\n",
    "    # x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(num_factors)(x_body) # from logits true will automatically add the sigmoid\n",
    "\n",
    "    model = keras.models.Model(\n",
    "        inputs = [subjectInput, bodyInput],\n",
    "        outputs = x,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        loss=keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=[\n",
    "            'acc',\n",
    "            # pr curve area - should help with imbalanced data\n",
    "            keras.metrics.AUC(curve='PR', name='auc'),\n",
    "            # keras.metrics.TruePositives(name = 'tp'),\n",
    "            # keras.metrics.FalsePositives(name = 'fp'),\n",
    "            # keras.metrics.TrueNegatives(name = 'tn'),\n",
    "            # keras.metrics.FalseNegatives(name = 'fn'),\n",
    "            keras.metrics.Precision(name='prec'),\n",
    "            keras.metrics.Recall(name='rec'),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return model \n",
    "\n",
    "multilabel_model = build_model(\n",
    "    subject_length = SUBJECT_LEN,\n",
    "    body_length = SEQUENCE_LEN,\n",
    "    embedding = embedding,\n",
    "    num_factors = NUM_FACTORS\n",
    ")\n",
    "\n",
    "multilabel_model = train_and_evaluate(\n",
    "    model = multilabel_model,\n",
    "    train_ds = ([train_email_titles, train_email_bodies], train_y),\n",
    "    test_ds = ([test_email_titles, test_email_bodies], test_y),\n",
    "    valid_ds = ([valid_email_titles, valid_email_bodies], valid_y),\n",
    "    epochs = 500,\n",
    "    batch_size = 32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type((train_x, train_y)) is tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
